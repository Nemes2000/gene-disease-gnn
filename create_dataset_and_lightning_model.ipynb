{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X5DDeTeTJjYM",
    "outputId": "f5235074-31ad-4a76-d4c0-ad3e20e14498"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1f561b2f5b0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from torch_geometric.data import Dataset, Data\n",
    "from torch_geometric.transforms import NormalizeFeatures\n",
    "\n",
    "from torchmetrics import ConfusionMatrix, AUROC, F1Score, Precision, Recall\n",
    "\n",
    "# PyTorch Lightning\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch import Tensor\n",
    "\n",
    "# PyTorch geometric\n",
    "import torch_geometric\n",
    "import torch_geometric.data as geom_data\n",
    "import torch_geometric.nn as geom_nn\n",
    "\n",
    "#Wandb\n",
    "import wandb\n",
    "\n",
    "# PL callbacks\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Ensure that all operations are deterministic on GPU (if used) for reproducibility\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "pl.seed_everything(42)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "s2cLFGzKJjYP"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mnemes-attila\u001b[0m (\u001b[33mnemes-attila-budapesti-m-szaki-s-gazdas-gtudom-nyi-egyetem\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: C:\\Users\\Nemes\\_netrc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MIN_DISEASE_S_GENE_NUMBER = 0\n",
    "TEST_TRAIN_SPLIT = 0.5\n",
    "TEST_VAL_SPLIT = 0.5\n",
    "\n",
    "EPOCHS = 10\n",
    "AVAIL_GPUS = min(1, torch.cuda.device_count())\n",
    "\n",
    "CHECKPOINT_PATH = os.environ.get(\"PATH_CHECKPOINT\", \"data/saved_models/\")\n",
    "gnn_layer_by_name = {\"GCN\": geom_nn.GCNConv, \"GAT\": geom_nn.GATConv, \"GraphConv\": geom_nn.GraphConv}\n",
    "\n",
    "wandb.login(key=\"e1f878235d3945d4141f9f8e5af41d712fca6eba\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "A4x0_KzLJjYP"
   },
   "outputs": [],
   "source": [
    "class IdMapper():\n",
    "    sorted_diseases = []\n",
    "    sorted_genes = []\n",
    "\n",
    "    def __init__(self, gene_file, disease_file):\n",
    "        genes = pd.read_csv(gene_file, sep=\"\\t\")\n",
    "        self.genes = genes[\"genes\"].sort_values().unique()\n",
    "\n",
    "        disieses = pd.read_csv(disease_file, sep=\"\\t\")\n",
    "        diseases_filtered = disieses.groupby(\"diseaseId\").filter(lambda x: len(x) > MIN_DISEASE_S_GENE_NUMBER)\n",
    "        self.diseases = diseases_filtered[\"diseaseId\"].sort_values().unique()\n",
    "\n",
    "    def diseases_idx_to_id_map(self):\n",
    "        return { idx: item  for idx, item in enumerate(self.diseases)}\n",
    "\n",
    "    def diseases_id_to_idx_map(self):\n",
    "        return { item: idx  for idx, item in enumerate(self.diseases)}\n",
    "\n",
    "    def genes_idx_to_id_map(self):\n",
    "        return { idx: item  for idx, item in enumerate(self.genes)}\n",
    "\n",
    "    def genes_id_to_idx_map(self):\n",
    "        return { item: idx  for idx, item in enumerate(self.genes)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "PZTinNMXJjYQ"
   },
   "outputs": [],
   "source": [
    "class GeneDataset(Dataset):\n",
    "    def __init__(self, root, filenames, test_size, val_size, test=False, transform=None, pre_transform=None):\n",
    "        \"\"\"\n",
    "        root = Where the dataset should be stored. This folder is split\n",
    "        into raw_dir (downloaded dataset) and processed_dir (processed data).\n",
    "        \"\"\"\n",
    "        self.test = test\n",
    "        self.test_size = test_size\n",
    "        self.val_size = val_size\n",
    "        self.filenames = filenames\n",
    "        self.mapper = IdMapper(\"./data/raw/\"+filenames[0], \"./data/raw/\"+filenames[2])\n",
    "        super(GeneDataset, self).__init__(root, transform, pre_transform)\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        \"\"\" If this file exists in raw_dir, the download is not triggered.\n",
    "            (The download func. is not implemented here)\n",
    "        \"\"\"\n",
    "        return self.filenames\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        \"\"\" If these files are found in raw_dir, processing is skipped\"\"\"\n",
    "        if self.test:\n",
    "            return [F'{file_name}_test' for file_name in self.raw_paths]\n",
    "        else:\n",
    "            return self.raw_paths\n",
    "\n",
    "    def download(self):\n",
    "        pass\n",
    "\n",
    "    def process(self):\n",
    "        self.genes_features = pd.read_csv(self.raw_paths[0], sep=\"\\t\")\n",
    "        self.edges_features = pd.read_csv(self.raw_paths[1], sep=\"\\t\")\n",
    "        self.disiese_gene_matrix = pd.read_csv(self.raw_paths[2], sep=\"\\t\")\n",
    "\n",
    "        self.genes = self.genes_features[\"genes\"].sort_values().unique()\n",
    "        self.diseases = self.disiese_gene_matrix[\"diseaseId\"].sort_values().unique()\n",
    "\n",
    "        node_feats = self._get_node_features(self.genes_features)\n",
    "        edge_feats = self._get_edge_features(self.edges_features)\n",
    "        edge_index = self._get_adjacency_info(self.edges_features)\n",
    "\n",
    "        y = self._create_mask_matrix(self.disiese_gene_matrix.copy()).to(torch.float32)\n",
    "        train_mask, validation_mask, test_mask = self._get_train_val_test_mask(self.disiese_gene_matrix.copy())\n",
    "\n",
    "        data = Data(x=node_feats,\n",
    "                    edge_index=edge_index,\n",
    "                    edge_weight=edge_feats,\n",
    "                    test_mask=test_mask, val_mask=validation_mask, train_mask=train_mask, y=y)\n",
    "\n",
    "        if self.test:\n",
    "            torch.save(data, os.path.join(self.processed_dir, 'graph_test.pt'))\n",
    "        else:\n",
    "            torch.save(data, os.path.join(self.processed_dir, 'graph.pt'))\n",
    "\n",
    "\n",
    "    def _get_train_val_test_mask(self, disiese_gene_matrix):\n",
    "        \"\"\"\n",
    "        i need too create matrices shape like disgenet\n",
    "        and in this matrix i pick random points which are gonna be the train mask, validation mask and test mask\n",
    "\n",
    "        in the train dataset i need to pick 80% from disgenet, equaly 0s and 1s in a column\n",
    "        in the validation dataset i need to pick 10% from disgenet, equaly 0s and 1s in a column\n",
    "        \"\"\"\n",
    "\n",
    "        train, validation, test = self._split_labels_to_train_val_test(disiese_gene_matrix)\n",
    "        disgenet_inverse = self._get_disgenet_inverse(disiese_gene_matrix)\n",
    "        train_n, validation_n, test_n = self._split_labels_to_train_val_test(disgenet_inverse)\n",
    "        train_r = pd.concat([train, train_n], ignore_index=True)\n",
    "        validation_r = pd.concat([validation, validation_n], ignore_index=True)\n",
    "        test_r = pd.concat([test, test_n], ignore_index=True)\n",
    "\n",
    "        train_mask = self._create_mask_matrix(train_r)\n",
    "        validation_mask = self._create_mask_matrix(validation_r)\n",
    "        test_mask = self._create_mask_matrix(test_r)\n",
    "\n",
    "        return train_mask, validation_mask, test_mask\n",
    "\n",
    "    def _split_labels_to_train_val_test(self, disgenet: pd.DataFrame):\n",
    "        #Split the positive targets to equal partitions by disease\n",
    "        disgenet_grouped = disgenet.groupby(by=\"diseaseId\", group_keys=False)\n",
    "        test_validation = disgenet_grouped.apply(lambda x: x.sample(frac=TEST_TRAIN_SPLIT, random_state=1))\n",
    "        train = disgenet.drop(test_validation.index)\n",
    "        test_validation_grouped = test_validation.groupby(by=\"diseaseId\", group_keys=False)\n",
    "\n",
    "        #Group by is needed before sample function call!!!\n",
    "        test = test_validation_grouped.apply(lambda x: x.sample(frac=TEST_VAL_SPLIT, random_state=1))\n",
    "        drop_indices = pd.concat([train, test]).index\n",
    "        validation = disgenet.drop(drop_indices)\n",
    "        return train, validation, test\n",
    "\n",
    "\n",
    "    def _get_disgenet_inverse(self, disgenet):\n",
    "        genes_frame = pd.DataFrame(list(self.genes), columns=[\"geneId\"])\n",
    "        diseases_frame = pd.DataFrame(self.diseases, columns=[\"diseaseId\"])\n",
    "        gene_disease_descartes_product = genes_frame.merge(diseases_frame, how=\"cross\")\n",
    "        disgenet_inverse = gene_disease_descartes_product.merge(disgenet, on=['geneId', 'diseaseId'], how='left', indicator=True)\n",
    "        return disgenet_inverse[disgenet_inverse['_merge'] == 'left_only'].drop(columns='_merge')\n",
    "\n",
    "\n",
    "    def _create_mask_matrix(self, dataframe):\n",
    "        dataframe_for_matrix = pd.DataFrame(np.zeros((len(self.genes), len(self.diseases)),))\n",
    "        gene_id_to_idx = self.mapper.genes_id_to_idx_map()\n",
    "        disease_id_to_idx = self.mapper.diseases_id_to_idx_map()\n",
    "\n",
    "        dataframe[\"geneId\"] = dataframe[\"geneId\"].map(gene_id_to_idx)\n",
    "        dataframe[\"diseaseId\"] = dataframe[\"diseaseId\"].map(disease_id_to_idx)\n",
    "        tuples_array = [row for row in dataframe.itertuples(index=False, name=None)]\n",
    "        for row, col in tqdm(tuples_array):\n",
    "            dataframe_for_matrix.loc[row, col] = 1\n",
    "\n",
    "        return torch.tensor(dataframe_for_matrix.to_numpy(), dtype=torch.bool)\n",
    "\n",
    "    def _get_node_features(self, genes):\n",
    "        gene_id_to_idx = self.mapper.genes_id_to_idx_map()\n",
    "        genes[\"genes\"] = self.genes_features[\"genes\"].map(gene_id_to_idx)\n",
    "        all_node_feats = genes.values.tolist()\n",
    "        all_node_feats = np.asarray(all_node_feats)\n",
    "\n",
    "        return torch.tensor(all_node_feats, dtype=torch.float32)\n",
    "\n",
    "    def _get_edge_features(self, edges):\n",
    "        \"\"\"\n",
    "        This will return a matrix / 2d array of the shape\n",
    "        [Number of edges, Edge Feature size]\n",
    "        \"\"\"\n",
    "        duplicated_edges = edges.loc[edges.index.repeat(2)].reset_index(drop=True)\n",
    "        all_edge_feats = duplicated_edges[\"combined_score\"].tolist()\n",
    "        return torch.tensor(all_edge_feats, dtype=torch.float32)\n",
    "\n",
    "\n",
    "    def _get_adjacency_info(self, edges):\n",
    "        \"\"\"\n",
    "        We want to be sure that the order of the indices\n",
    "        matches the order of the edge features\n",
    "        \"\"\"\n",
    "        gene_id_to_idx = self.mapper.genes_id_to_idx_map()\n",
    "\n",
    "        edge_indices = []\n",
    "        gene_1 = edges[\"gene1\"].map(gene_id_to_idx)\n",
    "        gene_2 = edges[\"gene2\"].map(gene_id_to_idx)\n",
    "        edges = pd.concat([gene_1, gene_2], axis=1).values.tolist()\n",
    "\n",
    "        #iterate over the edges end duplicate it because for one edge we need: n1,n2 and n2,n1\n",
    "        double_edges = []\n",
    "        for edge in edges:\n",
    "            double_edges += [ edge, [edge[1], edge[0]]]\n",
    "\n",
    "        edge_indices = torch.tensor(double_edges)\n",
    "        edge_indices = edge_indices.t().to(torch.int32).view(2, -1)\n",
    "        return edge_indices\n",
    "\n",
    "    def len(self):\n",
    "        return self.genes.shape[0]\n",
    "\n",
    "    def get(self, idx):\n",
    "        \"\"\" - Equivalent to __getitem__ in pytorch\n",
    "            - Is not needed for PyG's InMemoryDataset\n",
    "        \"\"\"\n",
    "        if self.test:\n",
    "            graph = torch.load(os.path.join(self.processed_dir, 'graph_test.pt'), weights_only=False)\n",
    "        else:\n",
    "            graph = torch.load(os.path.join(self.processed_dir, 'graph.pt'), weights_only=False)\n",
    "\n",
    "        return graph\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.get(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5n9RMMnPJjYQ",
    "outputId": "9fcf4a6b-612c-45cf-f0f9-66c1f0b7575e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing...\n",
      "100%|██████████| 11/11 [00:00<00:00, 10966.80it/s]\n",
      "C:\\Users\\Nemes\\AppData\\Local\\Temp\\ipykernel_15056\\3127019212.py:83: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  test_validation = disgenet_grouped.apply(lambda x: x.sample(frac=TEST_TRAIN_SPLIT, random_state=1))\n",
      "C:\\Users\\Nemes\\AppData\\Local\\Temp\\ipykernel_15056\\3127019212.py:88: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  test = test_validation_grouped.apply(lambda x: x.sample(frac=TEST_VAL_SPLIT, random_state=1))\n",
      "C:\\Users\\Nemes\\AppData\\Local\\Temp\\ipykernel_15056\\3127019212.py:83: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  test_validation = disgenet_grouped.apply(lambda x: x.sample(frac=TEST_TRAIN_SPLIT, random_state=1))\n",
      "C:\\Users\\Nemes\\AppData\\Local\\Temp\\ipykernel_15056\\3127019212.py:88: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  test = test_validation_grouped.apply(lambda x: x.sample(frac=TEST_VAL_SPLIT, random_state=1))\n",
      "100%|██████████| 34/34 [00:00<00:00, 2169.18it/s]\n",
      "100%|██████████| 11/11 [00:00<?, ?it/s]\n",
      "100%|██████████| 9/9 [00:00<?, ?it/s]\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "dataset = GeneDataset(\n",
    "    root=\"./data\",\n",
    "    filenames=[\"gtex_genes_test.csv\", \"gene_graph_test.csv\", \"disgenet_test.csv\"],\n",
    "    test_size=0.2,\n",
    "    val_size=0.0,\n",
    "    transform=NormalizeFeatures())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l1ruIfKEJjYQ"
   },
   "source": [
    "disgenetet úgy tovább szűrni, hogy az egyes betegséghez legalább x gén tartozzon --> végén majd kiprobálni, hogy nem szürök rajtuk\n",
    "\n",
    "GCN --> a veszteség függvény legyen jó, sima bináris osztályozás"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qz2v_V-FJjYR"
   },
   "source": [
    "keresztvalidáció\n",
    "\n",
    "mátrixokkal dolgozzak\n",
    "\n",
    "ha kiegyensulyozatlan akkor --> f1 score, avg precision, precision-recall görbe, (olyan metrikákat használjak)\n",
    "                                    dúsitást NEEEE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "J0PqCdirJjYS"
   },
   "outputs": [],
   "source": [
    "class GCNLayer(torch.nn.Module):\n",
    "    def __init__(self, c_in, c_out):\n",
    "        super().__init__()\n",
    "        self.projection = torch.nn.Linear(c_in, c_out)\n",
    "\n",
    "    def forward(self, node_feats, adj_matrix):\n",
    "        \"\"\"Forward.\n",
    "\n",
    "        Args:\n",
    "            node_feats: Tensor with node features of shape [batch_size, num_nodes, c_in]\n",
    "            adj_matrix: Batch of adjacency matrices of the graph. If there is an edge from i to j,\n",
    "                         adj_matrix[b,i,j]=1 else 0. Supports directed edges by non-symmetric matrices.\n",
    "                         Assumes to already have added the identity connections.\n",
    "                         Shape: [batch_size, num_nodes, num_nodes]\n",
    "\n",
    "        \"\"\"\n",
    "        # Num neighbours = number of incoming edges\n",
    "        num_neighbours = adj_matrix.sum(dim=-1, keepdims=True)\n",
    "        node_feats = self.projection(node_feats)\n",
    "        node_feats = torch.bmm(adj_matrix, node_feats)\n",
    "        node_feats = node_feats / num_neighbours\n",
    "        return node_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "aVFOCGs4JjYT"
   },
   "outputs": [],
   "source": [
    "class GATLayer(nn.Module):\n",
    "    def __init__(self, c_in, c_out, num_heads=1, concat_heads=True, alpha=0.2):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            c_in: Dimensionality of input features\n",
    "            c_out: Dimensionality of output features\n",
    "            num_heads: Number of heads, i.e. attention mechanisms to apply in parallel. The\n",
    "                        output features are equally split up over the heads if concat_heads=True.\n",
    "            concat_heads: If True, the output of the different heads is concatenated instead of averaged.\n",
    "            alpha: Negative slope of the LeakyReLU activation.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.concat_heads = concat_heads\n",
    "        if self.concat_heads:\n",
    "            assert c_out % num_heads == 0, \"Number of output features must be a multiple of the count of heads.\"\n",
    "            c_out = c_out // num_heads\n",
    "\n",
    "        # Sub-modules and parameters needed in the layer\n",
    "        self.projection = nn.Linear(c_in, c_out * num_heads)\n",
    "        self.a = nn.Parameter(Tensor(num_heads, 2 * c_out))  # One per head\n",
    "        self.leakyrelu = nn.LeakyReLU(alpha)\n",
    "\n",
    "        # Initialization from the original implementation\n",
    "        nn.init.xavier_uniform_(self.projection.weight.data, gain=1.414)\n",
    "        nn.init.xavier_uniform_(self.a.data, gain=1.414)\n",
    "\n",
    "    def forward(self, node_feats, adj_matrix, print_attn_probs=False):\n",
    "        \"\"\"Forward.\n",
    "\n",
    "        Args:\n",
    "            node_feats: Input features of the node. Shape: [batch_size, c_in]\n",
    "            adj_matrix: Adjacency matrix including self-connections. Shape: [batch_size, num_nodes, num_nodes]\n",
    "            print_attn_probs: If True, the attention weights are printed during the forward pass\n",
    "                               (for debugging purposes)\n",
    "\n",
    "        \"\"\"\n",
    "        batch_size, num_nodes = node_feats.size(0), node_feats.size(1)\n",
    "\n",
    "        # Apply linear layer and sort nodes by head\n",
    "        node_feats = self.projection(node_feats)\n",
    "        node_feats = node_feats.view(batch_size, num_nodes, self.num_heads, -1)\n",
    "\n",
    "        # We need to calculate the attention logits for every edge in the adjacency matrix\n",
    "        # Doing this on all possible combinations of nodes is very expensive\n",
    "        # => Create a tensor of [W*h_i||W*h_j] with i and j being the indices of all edges\n",
    "        # Returns indices where the adjacency matrix is not 0 => edges\n",
    "        edges = adj_matrix.nonzero(as_tuple=False)\n",
    "        node_feats_flat = node_feats.view(batch_size * num_nodes, self.num_heads, -1)\n",
    "        edge_indices_row = edges[:, 0] * num_nodes + edges[:, 1]\n",
    "        edge_indices_col = edges[:, 0] * num_nodes + edges[:, 2]\n",
    "        a_input = torch.cat(\n",
    "            [\n",
    "                torch.index_select(input=node_feats_flat, index=edge_indices_row, dim=0),\n",
    "                torch.index_select(input=node_feats_flat, index=edge_indices_col, dim=0),\n",
    "            ],\n",
    "            dim=-1,\n",
    "        )  # Index select returns a tensor with node_feats_flat being indexed at the desired positions\n",
    "\n",
    "        # Calculate attention MLP output (independent for each head)\n",
    "        attn_logits = torch.einsum(\"bhc,hc->bh\", a_input, self.a)\n",
    "        attn_logits = self.leakyrelu(attn_logits)\n",
    "\n",
    "        # Map list of attention values back into a matrix\n",
    "        attn_matrix = attn_logits.new_zeros(adj_matrix.shape + (self.num_heads,)).fill_(-9e15)\n",
    "        attn_matrix[adj_matrix[..., None].repeat(1, 1, 1, self.num_heads) == 1] = attn_logits.reshape(-1)\n",
    "\n",
    "        # Weighted average of attention\n",
    "        attn_probs = F.softmax(attn_matrix, dim=2)\n",
    "        if print_attn_probs:\n",
    "            print(\"Attention probs\\n\", attn_probs.permute(0, 3, 1, 2))\n",
    "        node_feats = torch.einsum(\"bijh,bjhc->bihc\", attn_probs, node_feats)\n",
    "\n",
    "        # If heads should be concatenated, we can do this by reshaping. Otherwise, take mean\n",
    "        if self.concat_heads:\n",
    "            node_feats = node_feats.reshape(batch_size, num_nodes, -1)\n",
    "        else:\n",
    "            node_feats = node_feats.mean(dim=2)\n",
    "\n",
    "        return node_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "gaeiw3EuJjYT"
   },
   "outputs": [],
   "source": [
    "class GNNModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        c_in,\n",
    "        c_hidden,\n",
    "        c_out,\n",
    "        num_layers=2,\n",
    "        layer_name=\"GCN\",\n",
    "        dp_rate=0.1,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        \"\"\"GNNModel.\n",
    "\n",
    "        Args:\n",
    "            c_in: Dimension of input features\n",
    "            c_hidden: Dimension of hidden features\n",
    "            c_out: Dimension of the output features. Usually number of classes in classification\n",
    "            num_layers: Number of \"hidden\" graph layers\n",
    "            layer_name: String of the graph layer to use\n",
    "            dp_rate: Dropout rate to apply throughout the network\n",
    "            kwargs: Additional arguments for the graph layer (e.g. number of heads for GAT)\n",
    "\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        gnn_layer = gnn_layer_by_name[layer_name]\n",
    "\n",
    "        layers = []\n",
    "        in_channels, out_channels = c_in, c_hidden\n",
    "        for l_idx in range(num_layers - 1):\n",
    "            layers += [\n",
    "                gnn_layer(in_channels=in_channels, out_channels=out_channels, **kwargs),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Dropout(dp_rate),\n",
    "            ]\n",
    "            in_channels = c_hidden\n",
    "        layers += [gnn_layer(in_channels=in_channels, out_channels=c_out, **kwargs)]\n",
    "        self.layers = nn.ModuleList(layers)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_weight):\n",
    "        \"\"\"Forward.\n",
    "\n",
    "        Args:\n",
    "            x: Input features per node\n",
    "            edge_index: List of vertex index pairs representing the edges in the graph (PyTorch geometric notation)\n",
    "\n",
    "        \"\"\"\n",
    "        for layer in self.layers:\n",
    "            # For graph layers, we need to add the \"edge_index\" tensor as additional input\n",
    "            # All PyTorch Geometric graph layer inherit the class \"MessagePassing\", hence\n",
    "            # we can simply check the class type.\n",
    "            if isinstance(layer, geom_nn.MessagePassing):\n",
    "                x = layer(x, edge_index, edge_weight)\n",
    "            else:\n",
    "                x = layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "SLFxDgu3JjYT"
   },
   "outputs": [],
   "source": [
    "# The simple GCN modell\n",
    "class TestGCN(pl.LightningModule):\n",
    "    def __init__(self, model_name, **model_kwargs):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.loss_module = nn.CrossEntropyLoss()\n",
    "        self.model = GNNModel(**model_kwargs)\n",
    "\n",
    "        self.learning_rate=0.01\n",
    "        self.decay=5e-4\n",
    "\n",
    "        self.cm = ConfusionMatrix(task=\"binary\", num_classes=2)\n",
    "        self.aucroc = AUROC(task=\"binary\", num_classes=2)\n",
    "        self.f1 = F1Score(task=\"binary\", num_classes=2)\n",
    "        self.precision = Precision(task=\"binary\", num_classes=2)\n",
    "        self.recall = Recall(task=\"binary\", num_classes=2)\n",
    "\n",
    "    def forward(self, data, mode=\"train\"):\n",
    "        x, edge_index, edge_weight = data.x, data.edge_index, data.edge_weight\n",
    "        new_x = self.model(x, edge_index, edge_weight)\n",
    "\n",
    "        # Only calculate the loss on the nodes corresponding to the mask\n",
    "        if mode == \"train\":\n",
    "            mask = data.train_mask\n",
    "        elif mode == \"val\":\n",
    "            mask = data.val_mask\n",
    "        elif mode == \"test\":\n",
    "            mask = data.test_mask\n",
    "        else:\n",
    "            assert False, f\"Unknown forward mode: {mode}\"\n",
    "\n",
    "        loss = self.loss_module(new_x[mask], data.y[mask])\n",
    "        acc = (new_x[mask].argmax(dim=-1) == data.y[mask]).sum().float() / mask.sum()\n",
    "\n",
    "        if mode == \"test\":\n",
    "            return loss, acc, new_x\n",
    "        return loss, acc\n",
    "\n",
    "    def training_step(self, data):\n",
    "        loss, acc = self.forward(data, mode=\"train\")\n",
    "        self.log('train_loss', loss, prog_bar=True, on_epoch=True)\n",
    "        self.log('train_acc', acc, prog_bar=True, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, data: GeneDataset):\n",
    "        loss, acc = self.forward(data, mode=\"val\")\n",
    "        self.log(\"val_acc\", acc)\n",
    "        self.log(\"val_loss\", loss)\n",
    "\n",
    "    def test_step(self, data: GeneDataset):\n",
    "        loss, acc, x = self.forward(data, mode=\"test\")\n",
    "        x_masked = x[data.test_mask]\n",
    "        y_masked = data.y[data.test_mask]\n",
    "\n",
    "        self.log(\"test_acc\", acc)\n",
    "        self.log('test_loss', loss, prog_bar=True, on_epoch=True)\n",
    "        self.cm.update(x_masked, y_masked)\n",
    "        self.aucroc.update(x_masked, y_masked)\n",
    "        self.f1.update(x_masked, y_masked)\n",
    "        self.precision.update(x_masked, y_masked)\n",
    "        self.recall.update(x_masked, y_masked)\n",
    "        return loss\n",
    "\n",
    "    def on_test_epoch_end(self) -> None:\n",
    "        self.cm.plot()\n",
    "        self.log('test_auc_roc', self.aucroc.compute(), prog_bar=True, on_epoch=True)\n",
    "        self.log('test_f1', self.f1.compute(), prog_bar=True, on_epoch=True)\n",
    "        self.log('test_precision', self.precision.compute(), prog_bar=True, on_epoch=True)\n",
    "        self.log('test_recall', self.recall.compute(), prog_bar=True, on_epoch=True)\n",
    "        return super().on_test_epoch_end()\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.learning_rate, weight_decay=self.decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CSLwdHANJjYU"
   },
   "outputs": [],
   "source": [
    "def train_node_classifier(model_name, dataset, **model_kwargs):\n",
    "    pl.seed_everything(42)\n",
    "    node_data_loader = geom_data.DataLoader(dataset, num_workers=11, persistent_workers=True)\n",
    "\n",
    "    # Create a PyTorch Lightning trainer\n",
    "    root_dir = os.path.join(CHECKPOINT_PATH, \"TestGCN\" + model_name)\n",
    "    os.makedirs(root_dir, exist_ok=True)\n",
    "    trainer = pl.Trainer(\n",
    "        default_root_dir=root_dir,\n",
    "        callbacks=[ModelCheckpoint(save_weights_only=True, mode=\"max\", monitor=\"val_acc\")],\n",
    "        accelerator=\"auto\",\n",
    "        devices=1,\n",
    "        max_epochs=EPOCHS,\n",
    "        enable_progress_bar=False,\n",
    "        logger=pl.loggers.WandbLogger(project=\"gene-disease-test\", log_model=\"all\")\n",
    "    )\n",
    "    trainer.logger._default_hp_metric = None  # Optional logging argument that we don't need\n",
    "\n",
    "    # Check whether pretrained model exists. If yes, load it and skip training\n",
    "    # pretrained_filename = os.path.join(CHECKPOINT_PATH, f\"TestGCN{model_name}.ckpt\")\n",
    "    # if os.path.isfile(pretrained_filename):\n",
    "    #     print(\"Found pretrained model, loading...\")\n",
    "    #     model = TestGCN.load_from_checkpoint(pretrained_filename)\n",
    "    # else:\n",
    "    pl.seed_everything()\n",
    "    model = TestGCN(\n",
    "        model_name=model_name, c_in=dataset.num_node_features, c_out=dataset.num_classes, **model_kwargs\n",
    "    )\n",
    "    trainer.fit(model, node_data_loader, node_data_loader)\n",
    "    model = TestGCN.load_from_checkpoint(trainer.checkpoint_callback.best_model_path)\n",
    "\n",
    "    # Test best model on the test set\n",
    "    test_result = trainer.test(model, dataloaders=node_data_loader)\n",
    "    result = {\"test\": test_result[0][\"test_acc\"]}\n",
    "    wandb.finish()\n",
    "    return model, result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "TSrREhY7JjYU"
   },
   "outputs": [],
   "source": [
    "def print_results(result_dict):\n",
    "    if \"train\" in result_dict:\n",
    "        print(\"Train accuracy: %4.2f%%\" % (100.0 * result_dict[\"train\"]))\n",
    "    if \"val\" in result_dict:\n",
    "        print(\"Val accuracy:   %4.2f%%\" % (100.0 * result_dict[\"val\"]))\n",
    "    print(\"Test accuracy:  %4.2f%%\" % (100.0 * result_dict[\"test\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "PnxzXUgEJjYU",
    "outputId": "9d1bbad8-5ec8-46ee-f466-28659a9bf4a5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "d:\\winfiles\\miniconda3\\envs\\gen-disease-gnn\\Lib\\site-packages\\torch_geometric\\deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>.\\wandb\\run-20241120_185106-vowgamiv</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/nemes-attila-budapesti-m-szaki-s-gazdas-gtudom-nyi-egyetem/gene-disease-test/runs/vowgamiv' target=\"_blank\">hopeful-blaze-3</a></strong> to <a href='https://wandb.ai/nemes-attila-budapesti-m-szaki-s-gazdas-gtudom-nyi-egyetem/gene-disease-test' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/nemes-attila-budapesti-m-szaki-s-gazdas-gtudom-nyi-egyetem/gene-disease-test' target=\"_blank\">https://wandb.ai/nemes-attila-budapesti-m-szaki-s-gazdas-gtudom-nyi-egyetem/gene-disease-test</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/nemes-attila-budapesti-m-szaki-s-gazdas-gtudom-nyi-egyetem/gene-disease-test/runs/vowgamiv' target=\"_blank\">https://wandb.ai/nemes-attila-budapesti-m-szaki-s-gazdas-gtudom-nyi-egyetem/gene-disease-test/runs/vowgamiv</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name        | Type                  | Params | Mode \n",
      "--------------------------------------------------------------\n",
      "0 | loss_module | CrossEntropyLoss      | 0      | train\n",
      "1 | model       | GNNModel              | 329    | train\n",
      "2 | cm          | BinaryConfusionMatrix | 0      | train\n",
      "3 | aucroc      | BinaryAUROC           | 0      | train\n",
      "4 | f1          | BinaryF1Score         | 0      | train\n",
      "5 | precision   | BinaryPrecision       | 0      | train\n",
      "6 | recall      | BinaryRecall          | 0      | train\n",
      "--------------------------------------------------------------\n",
      "329       Trainable params\n",
      "0         Non-trainable params\n",
      "329       Total params\n",
      "0.001     Total estimated model params size (MB)\n",
      "16        Modules in train mode\n",
      "0         Modules in eval mode\n",
      "d:\\winfiles\\miniconda3\\envs\\gen-disease-gnn\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "d:\\winfiles\\miniconda3\\envs\\gen-disease-gnn\\Lib\\site-packages\\pytorch_lightning\\utilities\\data.py:78: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 6. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "d:\\winfiles\\miniconda3\\envs\\gen-disease-gnn\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "d:\\winfiles\\miniconda3\\envs\\gen-disease-gnn\\Lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:298: The number of training batches (6) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n",
      "d:\\winfiles\\miniconda3\\envs\\gen-disease-gnn\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "d:\\winfiles\\miniconda3\\envs\\gen-disease-gnn\\Lib\\site-packages\\torchmetrics\\utilities\\prints.py:43: UserWarning: No positive samples in targets, true positive value should be meaningless. Returning zero tensor in true positive score\n",
      "  warnings.warn(*args, **kwargs)  # noqa: B028\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "        test_acc                    0.0\n",
      "      test_auc_roc                  0.0\n",
      "         test_f1                    0.0\n",
      "        test_loss                   0.0\n",
      "     test_precision                 0.0\n",
      "       test_recall                  0.0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "        .wandb-row {\n",
       "            display: flex;\n",
       "            flex-direction: row;\n",
       "            flex-wrap: wrap;\n",
       "            justify-content: flex-start;\n",
       "            width: 100%;\n",
       "        }\n",
       "        .wandb-col {\n",
       "            display: flex;\n",
       "            flex-direction: column;\n",
       "            flex-basis: 100%;\n",
       "            flex: 1;\n",
       "            padding: 10px;\n",
       "        }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▂▃▃▄▄▅▅▅▅▆▆▇▇▇▇▇█</td></tr><tr><td>test_acc</td><td>▁</td></tr><tr><td>test_auc_roc</td><td>▁</td></tr><tr><td>test_f1</td><td>▁</td></tr><tr><td>test_loss</td><td>▁</td></tr><tr><td>test_precision</td><td>▁</td></tr><tr><td>test_recall</td><td>▁</td></tr><tr><td>train_acc_epoch</td><td>▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc_step</td><td>▁</td></tr><tr><td>train_loss_epoch</td><td>█▄▃▂▂▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇▇███</td></tr><tr><td>val_acc</td><td>███▁▁█████</td></tr><tr><td>val_loss</td><td>█▇▄▃▃▂▂▂▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>10</td></tr><tr><td>test_acc</td><td>0</td></tr><tr><td>test_auc_roc</td><td>0</td></tr><tr><td>test_f1</td><td>0</td></tr><tr><td>test_loss</td><td>0</td></tr><tr><td>test_precision</td><td>0</td></tr><tr><td>test_recall</td><td>0</td></tr><tr><td>train_acc_epoch</td><td>0</td></tr><tr><td>train_acc_step</td><td>0</td></tr><tr><td>train_loss_epoch</td><td>29.31233</td></tr><tr><td>train_loss_step</td><td>30.53066</td></tr><tr><td>trainer/global_step</td><td>60</td></tr><tr><td>val_acc</td><td>0.81818</td></tr><tr><td>val_loss</td><td>77.24193</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">hopeful-blaze-3</strong> at: <a href='https://wandb.ai/nemes-attila-budapesti-m-szaki-s-gazdas-gtudom-nyi-egyetem/gene-disease-test/runs/vowgamiv' target=\"_blank\">https://wandb.ai/nemes-attila-budapesti-m-szaki-s-gazdas-gtudom-nyi-egyetem/gene-disease-test/runs/vowgamiv</a><br/> View project at: <a href='https://wandb.ai/nemes-attila-budapesti-m-szaki-s-gazdas-gtudom-nyi-egyetem/gene-disease-test' target=\"_blank\">https://wandb.ai/nemes-attila-budapesti-m-szaki-s-gazdas-gtudom-nyi-egyetem/gene-disease-test</a><br/>Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20241120_185106-vowgamiv\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy:  0.00%\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "node_mlp_model, node_mlp_result = train_node_classifier(\n",
    "    model_name=\"GCN\", dataset=dataset, c_hidden=16, num_layers=2, dp_rate=0.1\n",
    ")\n",
    "\n",
    "print_results(node_mlp_result)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "gen-disease-gnn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
