{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X5DDeTeTJjYM",
    "outputId": "f5235074-31ad-4a76-d4c0-ad3e20e14498"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x262ce00adf0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch_geometric.data as torch_data\n",
    "from torch_geometric.transforms import NormalizeFeatures\n",
    "\n",
    "from torchmetrics import ConfusionMatrix, AUROC, F1Score, Precision, Recall\n",
    "\n",
    "# PyTorch Lightning\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch import Tensor\n",
    "\n",
    "# PyTorch geometric\n",
    "import torch_geometric\n",
    "import torch_geometric.data as geom_data\n",
    "import torch_geometric.nn as geom_nn\n",
    "from torch_geometric.loader import NeighborLoader\n",
    "\n",
    "#Wandb\n",
    "import wandb\n",
    "\n",
    "# PL callbacks\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Ensure that all operations are deterministic on GPU (if used) for reproducibility\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "pl.seed_everything(42)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s2cLFGzKJjYP"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: C:\\Users\\Nemes\\_netrc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MIN_DISEASE_S_GENE_NUMBER = 0\n",
    "TEST_TRAIN_SPLIT = 0.5\n",
    "TEST_VAL_SPLIT = 0.5\n",
    "\n",
    "EPOCHS = 10\n",
    "AVAIL_GPUS = min(1, torch.cuda.device_count())\n",
    "\n",
    "CHECKPOINT_PATH = os.environ.get(\"PATH_CHECKPOINT\", \"../data/saved_models/\")\n",
    "gnn_layer_by_name = {\"GCN\": geom_nn.GCNConv, \"GAT\": geom_nn.GATConv, \"GraphConv\": geom_nn.GraphConv}\n",
    "\n",
    "wandb.login(key=\"e1f878235d3945d4141f9f8e5af41d712fca6eba\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "A4x0_KzLJjYP"
   },
   "outputs": [],
   "source": [
    "class IdMapper():\n",
    "    sorted_diseases = []\n",
    "    sorted_genes = []\n",
    "\n",
    "    def __init__(self, gene_file, disease_file):\n",
    "        genes = pd.read_csv(gene_file, sep=\"\\t\")\n",
    "        self.genes = genes[\"genes\"].sort_values().unique()\n",
    "\n",
    "        disieses = pd.read_csv(disease_file, sep=\"\\t\")\n",
    "        diseases_filtered = disieses.groupby(\"diseaseId\").filter(lambda x: len(x) > MIN_DISEASE_S_GENE_NUMBER)\n",
    "        self.diseases = diseases_filtered[\"diseaseId\"].sort_values().unique()\n",
    "\n",
    "    def diseases_idx_to_id_map(self):\n",
    "        return { idx: item  for idx, item in enumerate(self.diseases)}\n",
    "\n",
    "    def diseases_id_to_idx_map(self):\n",
    "        return { item: idx  for idx, item in enumerate(self.diseases)}\n",
    "\n",
    "    def genes_idx_to_id_map(self):\n",
    "        return { idx: item  for idx, item in enumerate(self.genes)}\n",
    "\n",
    "    def genes_id_to_idx_map(self):\n",
    "        return { item: idx  for idx, item in enumerate(self.genes)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PZTinNMXJjYQ"
   },
   "outputs": [],
   "source": [
    "class GeneDataset(torch_data.Dataset):\n",
    "    def __init__(self, root, filenames, test_size, val_size, test=False, transform=None, pre_transform=None):\n",
    "        \"\"\"\n",
    "        root = Where the dataset should be stored. This folder is split\n",
    "        into raw_dir (downloaded dataset) and processed_dir (processed data).\n",
    "        \"\"\"\n",
    "        self.test = test\n",
    "        self.test_size = test_size\n",
    "        self.val_size = val_size\n",
    "        self.filenames = filenames\n",
    "        self.mapper = IdMapper(\"../data/raw/\"+filenames[0], \"../data/raw/\"+filenames[2])\n",
    "        super(GeneDataset, self).__init__(root, transform, pre_transform)\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        \"\"\" If this file exists in raw_dir, the download is not triggered.\n",
    "            (The download func. is not implemented here)\n",
    "        \"\"\"\n",
    "        return self.filenames\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        \"\"\" If these files are found in raw_dir, processing is skipped\"\"\"\n",
    "        if self.test:\n",
    "            return [F'{file_name}_test' for file_name in self.raw_paths]\n",
    "        else:\n",
    "            return self.raw_paths\n",
    "\n",
    "    def download(self):\n",
    "        pass\n",
    "\n",
    "    def process(self):\n",
    "        self.genes_features = pd.read_csv(self.raw_paths[0], sep=\"\\t\")\n",
    "        self.edges_features = pd.read_csv(self.raw_paths[1], sep=\"\\t\")\n",
    "        self.disiese_gene_matrix = pd.read_csv(self.raw_paths[2], sep=\"\\t\")\n",
    "\n",
    "        self.genes = self.genes_features[\"genes\"].sort_values().unique()\n",
    "        self.diseases = self.disiese_gene_matrix[\"diseaseId\"].sort_values().unique()\n",
    "\n",
    "        node_feats = self._get_node_features(self.genes_features)\n",
    "        edge_feats = self._get_edge_features(self.edges_features)\n",
    "        edge_index = self._get_adjacency_info(self.edges_features)\n",
    "\n",
    "        y = self._create_mask_matrix(self.disiese_gene_matrix.copy()).to(torch.float32)\n",
    "        train_mask, validation_mask, test_mask = self._get_train_val_test_mask(self.disiese_gene_matrix.copy())\n",
    "\n",
    "        data = torch_data.Data(x=node_feats,\n",
    "                    edge_index=edge_index,\n",
    "                    edge_attr=edge_feats,\n",
    "                    test_mask=test_mask, val_mask=validation_mask, train_mask=train_mask, y=y)\n",
    "\n",
    "        if self.test:\n",
    "            torch.save(data, os.path.join(self.processed_dir, 'graph_test.pt'))\n",
    "        else:\n",
    "            torch.save(data, os.path.join(self.processed_dir, 'graph.pt'))\n",
    "\n",
    "\n",
    "    def _get_train_val_test_mask(self, disiese_gene_matrix):\n",
    "        \"\"\"\n",
    "        i need too create matrices shape like disgenet\n",
    "        and in this matrix i pick random points which are gonna be the train mask, validation mask and test mask\n",
    "\n",
    "        in the train dataset i need to pick 80% from disgenet, equaly 0s and 1s in a column\n",
    "        in the validation dataset i need to pick 10% from disgenet, equaly 0s and 1s in a column\n",
    "        \"\"\"\n",
    "\n",
    "        train, validation, test = self._split_labels_to_train_val_test(disiese_gene_matrix)\n",
    "        disgenet_inverse = self._get_disgenet_inverse(disiese_gene_matrix)\n",
    "        train_n, validation_n, test_n = self._split_labels_to_train_val_test(disgenet_inverse)\n",
    "        train_r = pd.concat([train, train_n], ignore_index=True)\n",
    "        validation_r = pd.concat([validation, validation_n], ignore_index=True)\n",
    "        test_r = pd.concat([test, test_n], ignore_index=True)\n",
    "\n",
    "        train_mask = self._create_mask_matrix(train_r)\n",
    "        validation_mask = self._create_mask_matrix(validation_r)\n",
    "        test_mask = self._create_mask_matrix(test_r)\n",
    "\n",
    "        return train_mask, validation_mask, test_mask\n",
    "\n",
    "    def _split_labels_to_train_val_test(self, disgenet: pd.DataFrame):\n",
    "        #Split the positive targets to equal partitions by disease\n",
    "        disgenet_grouped = disgenet.groupby(by=\"diseaseId\", group_keys=False)\n",
    "        test_validation = disgenet_grouped.apply(lambda x: x.sample(frac=TEST_TRAIN_SPLIT, random_state=1))\n",
    "        train = disgenet.drop(test_validation.index)\n",
    "        test_validation_grouped = test_validation.groupby(by=\"diseaseId\", group_keys=False)\n",
    "\n",
    "        #Group by is needed before sample function call!!!\n",
    "        test = test_validation_grouped.apply(lambda x: x.sample(frac=TEST_VAL_SPLIT, random_state=1))\n",
    "        drop_indices = pd.concat([train, test]).index\n",
    "        validation = disgenet.drop(drop_indices)\n",
    "        return train, validation, test\n",
    "\n",
    "\n",
    "    def _get_disgenet_inverse(self, disgenet):\n",
    "        genes_frame = pd.DataFrame(list(self.genes), columns=[\"geneId\"])\n",
    "        diseases_frame = pd.DataFrame(self.diseases, columns=[\"diseaseId\"])\n",
    "        gene_disease_descartes_product = genes_frame.merge(diseases_frame, how=\"cross\")\n",
    "        disgenet_inverse = gene_disease_descartes_product.merge(disgenet, on=['geneId', 'diseaseId'], how='left', indicator=True)\n",
    "        return disgenet_inverse[disgenet_inverse['_merge'] == 'left_only'].drop(columns='_merge')\n",
    "\n",
    "\n",
    "    def _create_mask_matrix(self, dataframe):\n",
    "        dataframe_for_matrix = pd.DataFrame(np.zeros((len(self.genes), len(self.diseases)),))\n",
    "        gene_id_to_idx = self.mapper.genes_id_to_idx_map()\n",
    "        disease_id_to_idx = self.mapper.diseases_id_to_idx_map()\n",
    "\n",
    "        dataframe[\"geneId\"] = dataframe[\"geneId\"].map(gene_id_to_idx)\n",
    "        dataframe[\"diseaseId\"] = dataframe[\"diseaseId\"].map(disease_id_to_idx)\n",
    "        tuples_array = [row for row in dataframe.itertuples(index=False, name=None)]\n",
    "        for row, col in tqdm(tuples_array):\n",
    "            dataframe_for_matrix.loc[row, col] = 1\n",
    "\n",
    "        return torch.tensor(dataframe_for_matrix.to_numpy(), dtype=torch.bool)\n",
    "\n",
    "    def _get_node_features(self, genes):\n",
    "        gene_id_to_idx = self.mapper.genes_id_to_idx_map()\n",
    "        genes[\"genes\"] = self.genes_features[\"genes\"].map(gene_id_to_idx)\n",
    "        all_node_feats = genes.values.tolist()\n",
    "        all_node_feats = np.asarray(all_node_feats)\n",
    "\n",
    "        return torch.tensor(all_node_feats, dtype=torch.float32)\n",
    "\n",
    "    def _get_edge_features(self, edges):\n",
    "        \"\"\"\n",
    "        This will return a matrix / 2d array of the shape\n",
    "        [Number of edges, Edge Feature size]\n",
    "        \"\"\"\n",
    "        duplicated_edges = edges.loc[edges.index.repeat(2)].reset_index(drop=True)\n",
    "        all_edge_feats = duplicated_edges[\"combined_score\"].tolist()\n",
    "        return torch.tensor(all_edge_feats, dtype=torch.float32)\n",
    "\n",
    "\n",
    "    def _get_adjacency_info(self, edges):\n",
    "        \"\"\"\n",
    "        We want to be sure that the order of the indices\n",
    "        matches the order of the edge features\n",
    "        \"\"\"\n",
    "        gene_id_to_idx = self.mapper.genes_id_to_idx_map()\n",
    "\n",
    "        edge_indices = []\n",
    "        gene_1 = edges[\"gene1\"].map(gene_id_to_idx)\n",
    "        gene_2 = edges[\"gene2\"].map(gene_id_to_idx)\n",
    "        edges = pd.concat([gene_1, gene_2], axis=1).values.tolist()\n",
    "\n",
    "        #iterate over the edges end duplicate it because for one edge we need: n1,n2 and n2,n1\n",
    "        double_edges = []\n",
    "        for edge in edges:\n",
    "            double_edges += [ edge, [edge[1], edge[0]]]\n",
    "\n",
    "        edge_indices = torch.tensor(double_edges)\n",
    "        edge_indices = edge_indices.t().to(torch.int32).view(2, -1)\n",
    "        return edge_indices\n",
    "\n",
    "    def len(self):\n",
    "        return self.genes.shape[0]\n",
    "\n",
    "    def get(self, idx):\n",
    "        \"\"\" - Equivalent to __getitem__ in pytorch\n",
    "            - Is not needed for PyG's InMemoryDataset\n",
    "        \"\"\"\n",
    "        if self.test:\n",
    "            graph = torch.load(os.path.join(self.processed_dir, 'graph_test.pt'), weights_only=False)\n",
    "        else:\n",
    "            graph = torch.load(os.path.join(self.processed_dir, 'graph.pt'), weights_only=False)\n",
    "\n",
    "        return graph\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.get(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5n9RMMnPJjYQ",
    "outputId": "9fcf4a6b-612c-45cf-f0f9-66c1f0b7575e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing...\n",
      "100%|██████████| 11/11 [00:00<00:00, 7206.71it/s]\n",
      "C:\\Users\\Nemes\\AppData\\Local\\Temp\\ipykernel_13500\\886494690.py:83: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  test_validation = disgenet_grouped.apply(lambda x: x.sample(frac=TEST_TRAIN_SPLIT, random_state=1))\n",
      "C:\\Users\\Nemes\\AppData\\Local\\Temp\\ipykernel_13500\\886494690.py:88: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  test = test_validation_grouped.apply(lambda x: x.sample(frac=TEST_VAL_SPLIT, random_state=1))\n",
      "C:\\Users\\Nemes\\AppData\\Local\\Temp\\ipykernel_13500\\886494690.py:83: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  test_validation = disgenet_grouped.apply(lambda x: x.sample(frac=TEST_TRAIN_SPLIT, random_state=1))\n",
      "C:\\Users\\Nemes\\AppData\\Local\\Temp\\ipykernel_13500\\886494690.py:88: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  test = test_validation_grouped.apply(lambda x: x.sample(frac=TEST_VAL_SPLIT, random_state=1))\n",
      "100%|██████████| 34/34 [00:00<?, ?it/s]\n",
      "100%|██████████| 11/11 [00:00<00:00, 5066.70it/s]\n",
      "100%|██████████| 9/9 [00:00<?, ?it/s]\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "dataset = GeneDataset(\n",
    "    root=\"./data\",\n",
    "    filenames=[\"gtex_genes_test.csv\", \"gene_graph_test.csv\", \"disgenet_test.csv\"],\n",
    "    test_size=0.2,\n",
    "    val_size=0.5,\n",
    "    transform=NormalizeFeatures())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[6, 10], edge_index=[2, 8], edge_attr=[8], y=[6, 9], test_mask=[6, 9], val_mask=[6, 9], train_mask=[6, 9])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l1ruIfKEJjYQ"
   },
   "source": [
    "disgenetet úgy tovább szűrni, hogy az egyes betegséghez legalább x gén tartozzon --> végén majd kiprobálni, hogy nem szürök rajtuk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qz2v_V-FJjYR"
   },
   "source": [
    "keresztvalidáció\n",
    "\n",
    "ha kiegyensulyozatlan akkor --> f1 score, avg precision, precision-recall görbe, (olyan metrikákat használjak)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "J0PqCdirJjYS"
   },
   "outputs": [],
   "source": [
    "class GCNLayer(torch.nn.Module):\n",
    "    def __init__(self, c_in: int, c_out: int):\n",
    "        super().__init__()\n",
    "        self.projection = torch.nn.Linear(c_in, c_out)\n",
    "\n",
    "    def forward(self, node_feats, adj_matrix):\n",
    "        \"\"\"Forward.\n",
    "\n",
    "        Args:\n",
    "            node_feats: Tensor with node features of shape [batch_size, num_nodes, c_in]\n",
    "            adj_matrix: Batch of adjacency matrices of the graph. If there is an edge from i to j,\n",
    "                         adj_matrix[b,i,j]=1 else 0. Supports directed edges by non-symmetric matrices.\n",
    "                         Assumes to already have added the identity connections.\n",
    "                         Shape: [batch_size, num_nodes, num_nodes]\n",
    "\n",
    "        \"\"\"\n",
    "        # Num neighbours = number of incoming edges\n",
    "        num_neighbours = adj_matrix.sum(dim=-1, keepdims=True)\n",
    "        node_feats = self.projection(node_feats)\n",
    "        node_feats = torch.bmm(adj_matrix, node_feats)\n",
    "        node_feats = node_feats / num_neighbours\n",
    "        return node_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "aVFOCGs4JjYT"
   },
   "outputs": [],
   "source": [
    "class GATLayer(nn.Module):\n",
    "    def __init__(self, c_in, c_out, num_heads=1, concat_heads=True, alpha=0.2):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            c_in: Dimensionality of input features\n",
    "            c_out: Dimensionality of output features\n",
    "            num_heads: Number of heads, i.e. attention mechanisms to apply in parallel. The\n",
    "                        output features are equally split up over the heads if concat_heads=True.\n",
    "            concat_heads: If True, the output of the different heads is concatenated instead of averaged.\n",
    "            alpha: Negative slope of the LeakyReLU activation.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.concat_heads = concat_heads\n",
    "        if self.concat_heads:\n",
    "            assert c_out % num_heads == 0, \"Number of output features must be a multiple of the count of heads.\"\n",
    "            c_out = c_out // num_heads\n",
    "\n",
    "        # Sub-modules and parameters needed in the layer\n",
    "        self.projection = nn.Linear(c_in, c_out * num_heads)\n",
    "        self.a = nn.Parameter(Tensor(num_heads, 2 * c_out))  # One per head\n",
    "        self.leakyrelu = nn.LeakyReLU(alpha)\n",
    "\n",
    "        # Initialization from the original implementation\n",
    "        nn.init.xavier_uniform_(self.projection.weight.data, gain=1.414)\n",
    "        nn.init.xavier_uniform_(self.a.data, gain=1.414)\n",
    "\n",
    "    def forward(self, node_feats, adj_matrix, print_attn_probs=False):\n",
    "        \"\"\"Forward.\n",
    "\n",
    "        Args:\n",
    "            node_feats: Input features of the node. Shape: [batch_size, c_in]\n",
    "            adj_matrix: Adjacency matrix including self-connections. Shape: [batch_size, num_nodes, num_nodes]\n",
    "            print_attn_probs: If True, the attention weights are printed during the forward pass\n",
    "                               (for debugging purposes)\n",
    "\n",
    "        \"\"\"\n",
    "        batch_size, num_nodes = node_feats.size(0), node_feats.size(1)\n",
    "\n",
    "        # Apply linear layer and sort nodes by head\n",
    "        node_feats = self.projection(node_feats)\n",
    "        node_feats = node_feats.view(batch_size, num_nodes, self.num_heads, -1)\n",
    "\n",
    "        # We need to calculate the attention logits for every edge in the adjacency matrix\n",
    "        # Doing this on all possible combinations of nodes is very expensive\n",
    "        # => Create a tensor of [W*h_i||W*h_j] with i and j being the indices of all edges\n",
    "        # Returns indices where the adjacency matrix is not 0 => edges\n",
    "        edges = adj_matrix.nonzero(as_tuple=False)\n",
    "        node_feats_flat = node_feats.view(batch_size * num_nodes, self.num_heads, -1)\n",
    "        edge_indices_row = edges[:, 0] * num_nodes + edges[:, 1]\n",
    "        edge_indices_col = edges[:, 0] * num_nodes + edges[:, 2]\n",
    "        a_input = torch.cat(\n",
    "            [\n",
    "                torch.index_select(input=node_feats_flat, index=edge_indices_row, dim=0),\n",
    "                torch.index_select(input=node_feats_flat, index=edge_indices_col, dim=0),\n",
    "            ],\n",
    "            dim=-1,\n",
    "        )  # Index select returns a tensor with node_feats_flat being indexed at the desired positions\n",
    "\n",
    "        # Calculate attention MLP output (independent for each head)\n",
    "        attn_logits = torch.einsum(\"bhc,hc->bh\", a_input, self.a)\n",
    "        attn_logits = self.leakyrelu(attn_logits)\n",
    "\n",
    "        # Map list of attention values back into a matrix\n",
    "        attn_matrix = attn_logits.new_zeros(adj_matrix.shape + (self.num_heads,)).fill_(-9e15)\n",
    "        attn_matrix[adj_matrix[..., None].repeat(1, 1, 1, self.num_heads) == 1] = attn_logits.reshape(-1)\n",
    "\n",
    "        # Weighted average of attention\n",
    "        attn_probs = F.softmax(attn_matrix, dim=2)\n",
    "        if print_attn_probs:\n",
    "            print(\"Attention probs\\n\", attn_probs.permute(0, 3, 1, 2))\n",
    "        node_feats = torch.einsum(\"bijh,bjhc->bihc\", attn_probs, node_feats)\n",
    "\n",
    "        # If heads should be concatenated, we can do this by reshaping. Otherwise, take mean\n",
    "        if self.concat_heads:\n",
    "            node_feats = node_feats.reshape(batch_size, num_nodes, -1)\n",
    "        else:\n",
    "            node_feats = node_feats.mean(dim=2)\n",
    "\n",
    "        return node_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "gaeiw3EuJjYT"
   },
   "outputs": [],
   "source": [
    "class GNNModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        c_in,\n",
    "        c_hidden,\n",
    "        c_out,\n",
    "        num_layers=2,\n",
    "        layer_name=\"GCN\",\n",
    "        dropout_rate=0.1,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        \"\"\"GNNModel.\n",
    "\n",
    "        Args:\n",
    "            c_in: Dimension of input features\n",
    "            c_hidden: Dimension of hidden features\n",
    "            c_out: Dimension of the output features. Usually number of classes in classification\n",
    "            num_layers: Number of \"hidden\" graph layers\n",
    "            layer_name: String of the graph layer to use\n",
    "            dp_rate: Dropout rate to apply throughout the network\n",
    "            kwargs: Additional arguments for the graph layer (e.g. number of heads for GAT)\n",
    "\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        gnn_layer = gnn_layer_by_name[layer_name]\n",
    "\n",
    "        layers = []\n",
    "        in_channels, out_channels = c_in, c_hidden\n",
    "        for _ in range(num_layers - 1):\n",
    "            layers += [\n",
    "                gnn_layer(in_channels=in_channels, out_channels=out_channels, **kwargs),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Dropout(dropout_rate),\n",
    "            ]\n",
    "            in_channels = c_hidden\n",
    "        layers += [gnn_layer(in_channels=in_channels, out_channels=c_out, **kwargs)]\n",
    "        self.layers = nn.ModuleList(layers)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_weight):\n",
    "        \"\"\"Forward.\n",
    "\n",
    "        Args:\n",
    "            x: Input features per node\n",
    "            edge_index: List of vertex index pairs representing the edges in the graph (PyTorch geometric notation)\n",
    "\n",
    "        \"\"\"\n",
    "        for layer in self.layers:\n",
    "            # For graph layers, we need to add the \"edge_index\" tensor as additional input\n",
    "            # All PyTorch Geometric graph layer inherit the class \"MessagePassing\", hence\n",
    "            # we can simply check the class type.\n",
    "            if isinstance(layer, geom_nn.MessagePassing):\n",
    "                x = layer(x, edge_index, edge_weight)\n",
    "            else:\n",
    "                x = layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "SLFxDgu3JjYT"
   },
   "outputs": [],
   "source": [
    "# The simple GCN modell\n",
    "class TestGCN(pl.LightningModule):\n",
    "    def __init__(self, c_in, c_out, c_hidden, num_layers, dropout_rate):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.loss_module = nn.CrossEntropyLoss()\n",
    "        self.model = GNNModel(c_hidden=c_hidden, num_layers=num_layers, dropout_rate=dropout_rate, c_in=c_in, c_out=c_out)\n",
    "\n",
    "        self.learning_rate=0.01\n",
    "        self.decay=5e-4\n",
    "\n",
    "        self.cm = ConfusionMatrix(task=\"binary\", num_classes=2)\n",
    "        self.aucroc = AUROC(task=\"binary\", num_classes=2)\n",
    "        self.f1 = F1Score(task=\"binary\", num_classes=2)\n",
    "        self.precision = Precision(task=\"binary\", num_classes=2)\n",
    "        self.recall = Recall(task=\"binary\", num_classes=2)\n",
    "\n",
    "    def forward(self, data, mode=\"train\"):\n",
    "        x, edge_index, edge_weight = data.x, data.edge_index, data.edge_weight\n",
    "        new_x = self.model(x, edge_index, edge_weight)\n",
    "\n",
    "        # Only calculate the loss on the nodes corresponding to the mask\n",
    "        if mode == \"train\":\n",
    "            mask = data.train_mask\n",
    "        elif mode == \"val\":\n",
    "            mask = data.val_mask\n",
    "        elif mode == \"test\":\n",
    "            mask = data.test_mask\n",
    "        else:\n",
    "            assert False, f\"Unknown forward mode: {mode}\"\n",
    "\n",
    "        loss = self.loss_module(new_x[mask], data.y[mask])\n",
    "        acc = (new_x[mask].argmax(dim=-1) == data.y[mask]).sum().float() / mask.sum()\n",
    "\n",
    "        if mode == \"test\":\n",
    "            return loss, acc, new_x\n",
    "        return loss, acc\n",
    "\n",
    "    def training_step(self, data):\n",
    "        loss, acc = self.forward(data, mode=\"train\")\n",
    "        self.log('train_loss', loss, prog_bar=True, on_epoch=True)\n",
    "        self.log('train_acc', acc, prog_bar=True, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, data):\n",
    "        loss, acc = self.forward(data, mode=\"val\")\n",
    "        self.log(\"val_acc\", acc)\n",
    "        self.log(\"val_loss\", loss)\n",
    "\n",
    "    def test_step(self, data):\n",
    "        loss, acc, x = self.forward(data, mode=\"test\")\n",
    "        x_masked = x[data.test_mask]\n",
    "        y_masked = data.y[data.test_mask]\n",
    "\n",
    "        self.log(\"test_acc\", acc)\n",
    "        self.log('test_loss', loss, prog_bar=True, on_epoch=True)\n",
    "        self.cm.update(x_masked, y_masked)\n",
    "        self.aucroc.update(x_masked, y_masked)\n",
    "        self.f1.update(x_masked, y_masked)\n",
    "        self.precision.update(x_masked, y_masked)\n",
    "        self.recall.update(x_masked, y_masked)\n",
    "        return loss\n",
    "\n",
    "    def on_test_epoch_end(self) -> None:\n",
    "        self.cm.plot()\n",
    "        self.log('test_auc_roc', self.aucroc.compute(), prog_bar=True, on_epoch=True)\n",
    "        self.log('test_f1', self.f1.compute(), prog_bar=True, on_epoch=True)\n",
    "        self.log('test_precision', self.precision.compute(), prog_bar=True, on_epoch=True)\n",
    "        self.log('test_recall', self.recall.compute(), prog_bar=True, on_epoch=True)\n",
    "        return super().on_test_epoch_end()\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.learning_rate, weight_decay=self.decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How to use NeighborLoader ---> importani is to install librraries by this: https://github.com/pyg-team/pytorch_geometric/discussions/7866\n",
    "#                                          and use the propriate data : geom_data.Data\n",
    "#                                          and edge indexes must be long type\n",
    "# in_channels = 1\n",
    "# node_num = 8\n",
    "# features = [[i for j in range(in_channels)] for i in range(node_num)]\n",
    "# edge_index = torch.tensor([[2, 3, 3, 4, 5, 6, 7], [0, 0, 1, 1, 2, 3, 4]],\n",
    "#                           dtype=torch.long)\n",
    "# data = Data(torch.tensor(features), edge_index)\n",
    "# loader = NeighborLoader(data, [2], batch_size=1)\n",
    "# batch = next(iter(loader))\n",
    "# batch.edge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: fhwx9he8\n",
      "Sweep URL: https://wandb.ai/nemes-attila-budapesti-m-szaki-s-gazdas-gtudom-nyi-egyetem/gnn_test_logs/sweeps/fhwx9he8\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sweep_config = {\n",
    "    'method': 'random'\n",
    "}\n",
    "\n",
    "parameters_dict = {\n",
    "    'optimizer': {\n",
    "        'values': ['adam', 'sgd', 'adamW']\n",
    "    },\n",
    "    \"num_layers\": {\n",
    "        \"values\": [10,5,2]\n",
    "    },\n",
    "    'hidden_layer_size': {\n",
    "        'values': [30, 20, 16]\n",
    "    },\n",
    "    \"dropout_rate\": {\n",
    "        \"values\" : [0.5, 0.3, 0.1]\n",
    "    }\n",
    "}\n",
    "\n",
    "optimizer_map = {\n",
    "    'adam': torch.optim.Adam,\n",
    "    'sgd': torch.optim.SGD,\n",
    "    'adamW': torch.optim.AdamW\n",
    "}\n",
    "\n",
    "parameters_dict.update({\n",
    "    'learning_rate': {\n",
    "        'distribution': 'uniform',\n",
    "        'min': 0,\n",
    "        'max': 0.1\n",
    "      },\n",
    "    'batch_size': {\n",
    "        'distribution': 'q_log_uniform_values',\n",
    "        'q': 8,\n",
    "        'min': 64,\n",
    "        'max': 512\n",
    "      }\n",
    "   }\n",
    ")\n",
    "\n",
    "sweep_config['parameters'] = parameters_dict\n",
    "\n",
    "sweep_id = wandb.sweep(sweep_config, project=\"gnn_test_logs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cpkt = ModelCheckpoint(\n",
    "    monitor='val_acc',\n",
    "    mode='min',\n",
    "    save_top_k=1,\n",
    "    save_last=True,\n",
    "    dirpath='../data/saved_models/wandb',\n",
    "    filename='gnn_model')\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_acc',\n",
    "    mode='min',\n",
    "    patience=10,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "CSLwdHANJjYU"
   },
   "outputs": [],
   "source": [
    "def train_node_classifier(config, dataset, model_name=\"GCN\"):\n",
    "    pl.seed_everything(42)\n",
    "    \n",
    "    node_data_loader = geom_data.DataLoader(dataset)\n",
    "\n",
    "    # Create a PyTorch Lightning trainer\n",
    "    root_dir = os.path.join(CHECKPOINT_PATH, \"TestGCN\" + model_name)\n",
    "    os.makedirs(root_dir, exist_ok=True)\n",
    "    trainer = pl.Trainer(\n",
    "        default_root_dir=root_dir,\n",
    "        callbacks=[model_cpkt, early_stopping],\n",
    "        accelerator=\"auto\",\n",
    "        devices=1,\n",
    "        max_epochs=1,\n",
    "        enable_progress_bar=False,\n",
    "        logger=pl.loggers.WandbLogger(project=\"GCN_sweep_test\", log_model=\"all\")\n",
    "    )\n",
    "    trainer.logger._default_hp_metric = None  # Optional logging argument that we don't need\n",
    "\n",
    "    # Check whether pretrained model exists. If yes, load it and skip training\n",
    "    # pretrained_filename = os.path.join(CHECKPOINT_PATH, f\"TestGCN{model_name}.ckpt\")\n",
    "    # if os.path.isfile(pretrained_filename):\n",
    "    #     print(\"Found pretrained model, loading...\")\n",
    "    #     model = TestGCN.load_from_checkpoint(pretrained_filename)\n",
    "    # else:\n",
    "\n",
    "    c_hidden = config.hidden_layer_size\n",
    "    num_layers = config.num_layers\n",
    "    dropout_rate = config.dropout_rate\n",
    "\n",
    "    model = TestGCN(\n",
    "        c_in=dataset.num_node_features, c_out=dataset[0].y.shape[1], c_hidden=c_hidden, num_layers=num_layers, dropout_rate=dropout_rate\n",
    "    )\n",
    "\n",
    "    trainer.fit(model, node_data_loader, node_data_loader)\n",
    "\n",
    "    model = TestGCN.load_from_checkpoint(trainer.checkpoint_callback.best_model_path)\n",
    "\n",
    "    # Test best model on the test set\n",
    "    trainer.test(model, dataloaders=node_data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimalization_train(config=None):\n",
    "    with wandb.init(config=config):\n",
    "        config = wandb.config\n",
    "        train_node_classifier(config=config, model_name=\"GCN_sweep_test\", dataset=dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: k94m6p7f with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 112\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout_rate: 0.3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_layer_size: 30\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.022005905746128353\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_layers: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: sgd\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>d:\\gitrepos\\gene-disease-gnn\\wandb\\run-20241124_145541-k94m6p7f</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/nemes-attila-budapesti-m-szaki-s-gazdas-gtudom-nyi-egyetem/gnn_test_logs/runs/k94m6p7f' target=\"_blank\">major-sweep-3</a></strong> to <a href='https://wandb.ai/nemes-attila-budapesti-m-szaki-s-gazdas-gtudom-nyi-egyetem/gnn_test_logs' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/nemes-attila-budapesti-m-szaki-s-gazdas-gtudom-nyi-egyetem/gnn_test_logs/sweeps/fhwx9he8' target=\"_blank\">https://wandb.ai/nemes-attila-budapesti-m-szaki-s-gazdas-gtudom-nyi-egyetem/gnn_test_logs/sweeps/fhwx9he8</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/nemes-attila-budapesti-m-szaki-s-gazdas-gtudom-nyi-egyetem/gnn_test_logs' target=\"_blank\">https://wandb.ai/nemes-attila-budapesti-m-szaki-s-gazdas-gtudom-nyi-egyetem/gnn_test_logs</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/nemes-attila-budapesti-m-szaki-s-gazdas-gtudom-nyi-egyetem/gnn_test_logs/sweeps/fhwx9he8' target=\"_blank\">https://wandb.ai/nemes-attila-budapesti-m-szaki-s-gazdas-gtudom-nyi-egyetem/gnn_test_logs/sweeps/fhwx9he8</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/nemes-attila-budapesti-m-szaki-s-gazdas-gtudom-nyi-egyetem/gnn_test_logs/runs/k94m6p7f' target=\"_blank\">https://wandb.ai/nemes-attila-budapesti-m-szaki-s-gazdas-gtudom-nyi-egyetem/gnn_test_logs/runs/k94m6p7f</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "d:\\winfiles\\miniconda3\\envs\\gen-disease-gnn\\Lib\\site-packages\\torch_geometric\\deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "d:\\winfiles\\miniconda3\\envs\\gen-disease-gnn\\Lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:396: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "d:\\winfiles\\miniconda3\\envs\\gen-disease-gnn\\Lib\\site-packages\\pytorch_lightning\\callbacks\\model_checkpoint.py:654: Checkpoint directory D:\\gitrepos\\gene-disease-gnn\\data\\saved_models\\wandb exists and is not empty.\n",
      "\n",
      "  | Name        | Type                  | Params | Mode \n",
      "--------------------------------------------------------------\n",
      "0 | loss_module | CrossEntropyLoss      | 0      | train\n",
      "1 | model       | GNNModel              | 3.4 K  | train\n",
      "2 | cm          | BinaryConfusionMatrix | 0      | train\n",
      "3 | aucroc      | BinaryAUROC           | 0      | train\n",
      "4 | f1          | BinaryF1Score         | 0      | train\n",
      "5 | precision   | BinaryPrecision       | 0      | train\n",
      "6 | recall      | BinaryRecall          | 0      | train\n",
      "--------------------------------------------------------------\n",
      "3.4 K     Trainable params\n",
      "0         Non-trainable params\n",
      "3.4 K     Total params\n",
      "0.014     Total estimated model params size (MB)\n",
      "31        Modules in train mode\n",
      "0         Modules in eval mode\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_layers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dropout_rate' was locked by 'sweep' (ignored update).\n",
      "d:\\winfiles\\miniconda3\\envs\\gen-disease-gnn\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "d:\\winfiles\\miniconda3\\envs\\gen-disease-gnn\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "d:\\winfiles\\miniconda3\\envs\\gen-disease-gnn\\Lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:298: The number of training batches (6) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_layers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dropout_rate' was locked by 'sweep' (ignored update).\n",
      "d:\\winfiles\\miniconda3\\envs\\gen-disease-gnn\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "d:\\winfiles\\miniconda3\\envs\\gen-disease-gnn\\Lib\\site-packages\\torchmetrics\\utilities\\prints.py:43: UserWarning: No positive samples in targets, true positive value should be meaningless. Returning zero tensor in true positive score\n",
      "  warnings.warn(*args, **kwargs)  # noqa: B028\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_acc          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">            0.0            </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       test_auc_roc        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">            0.0            </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">          test_f1          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">            0.0            </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">            0.0            </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      test_precision       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">            0.0            </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">        test_recall        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">            0.0            </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_acc         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m           0.0           \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      test_auc_roc       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m           0.0           \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m         test_f1         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m           0.0           \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m           0.0           \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m     test_precision      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m           0.0           \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m       test_recall       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m           0.0           \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "        .wandb-row {\n",
       "            display: flex;\n",
       "            flex-direction: row;\n",
       "            flex-wrap: wrap;\n",
       "            justify-content: flex-start;\n",
       "            width: 100%;\n",
       "        }\n",
       "        .wandb-col {\n",
       "            display: flex;\n",
       "            flex-direction: column;\n",
       "            flex-basis: 100%;\n",
       "            flex: 1;\n",
       "            padding: 10px;\n",
       "        }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁█</td></tr><tr><td>test_acc</td><td>▁</td></tr><tr><td>test_auc_roc</td><td>▁</td></tr><tr><td>test_f1</td><td>▁</td></tr><tr><td>test_loss</td><td>▁</td></tr><tr><td>test_precision</td><td>▁</td></tr><tr><td>test_recall</td><td>▁</td></tr><tr><td>train_acc_epoch</td><td>▁</td></tr><tr><td>train_loss_epoch</td><td>▁</td></tr><tr><td>trainer/global_step</td><td>▁▁█</td></tr><tr><td>val_acc</td><td>▁</td></tr><tr><td>val_loss</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>1</td></tr><tr><td>test_acc</td><td>0</td></tr><tr><td>test_auc_roc</td><td>0</td></tr><tr><td>test_f1</td><td>0</td></tr><tr><td>test_loss</td><td>0</td></tr><tr><td>test_precision</td><td>0</td></tr><tr><td>test_recall</td><td>0</td></tr><tr><td>train_acc_epoch</td><td>0</td></tr><tr><td>train_loss_epoch</td><td>250.06923</td></tr><tr><td>trainer/global_step</td><td>6</td></tr><tr><td>val_acc</td><td>0</td></tr><tr><td>val_loss</td><td>11.49424</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">major-sweep-3</strong> at: <a href='https://wandb.ai/nemes-attila-budapesti-m-szaki-s-gazdas-gtudom-nyi-egyetem/gnn_test_logs/runs/k94m6p7f' target=\"_blank\">https://wandb.ai/nemes-attila-budapesti-m-szaki-s-gazdas-gtudom-nyi-egyetem/gnn_test_logs/runs/k94m6p7f</a><br/> View project at: <a href='https://wandb.ai/nemes-attila-budapesti-m-szaki-s-gazdas-gtudom-nyi-egyetem/gnn_test_logs' target=\"_blank\">https://wandb.ai/nemes-attila-budapesti-m-szaki-s-gazdas-gtudom-nyi-egyetem/gnn_test_logs</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20241124_145541-k94m6p7f\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: uhiszuws with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 160\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout_rate: 0.3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_layer_size: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.05191851021440519\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_layers: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: sgd\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>d:\\gitrepos\\gene-disease-gnn\\wandb\\run-20241124_145547-uhiszuws</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/nemes-attila-budapesti-m-szaki-s-gazdas-gtudom-nyi-egyetem/gnn_test_logs/runs/uhiszuws' target=\"_blank\">dark-sweep-4</a></strong> to <a href='https://wandb.ai/nemes-attila-budapesti-m-szaki-s-gazdas-gtudom-nyi-egyetem/gnn_test_logs' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/nemes-attila-budapesti-m-szaki-s-gazdas-gtudom-nyi-egyetem/gnn_test_logs/sweeps/fhwx9he8' target=\"_blank\">https://wandb.ai/nemes-attila-budapesti-m-szaki-s-gazdas-gtudom-nyi-egyetem/gnn_test_logs/sweeps/fhwx9he8</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/nemes-attila-budapesti-m-szaki-s-gazdas-gtudom-nyi-egyetem/gnn_test_logs' target=\"_blank\">https://wandb.ai/nemes-attila-budapesti-m-szaki-s-gazdas-gtudom-nyi-egyetem/gnn_test_logs</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/nemes-attila-budapesti-m-szaki-s-gazdas-gtudom-nyi-egyetem/gnn_test_logs/sweeps/fhwx9he8' target=\"_blank\">https://wandb.ai/nemes-attila-budapesti-m-szaki-s-gazdas-gtudom-nyi-egyetem/gnn_test_logs/sweeps/fhwx9he8</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/nemes-attila-budapesti-m-szaki-s-gazdas-gtudom-nyi-egyetem/gnn_test_logs/runs/uhiszuws' target=\"_blank\">https://wandb.ai/nemes-attila-budapesti-m-szaki-s-gazdas-gtudom-nyi-egyetem/gnn_test_logs/runs/uhiszuws</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "d:\\winfiles\\miniconda3\\envs\\gen-disease-gnn\\Lib\\site-packages\\torch_geometric\\deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "d:\\winfiles\\miniconda3\\envs\\gen-disease-gnn\\Lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:396: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "d:\\winfiles\\miniconda3\\envs\\gen-disease-gnn\\Lib\\site-packages\\pytorch_lightning\\callbacks\\model_checkpoint.py:654: Checkpoint directory D:\\gitrepos\\gene-disease-gnn\\data\\saved_models\\wandb exists and is not empty.\n",
      "\n",
      "  | Name        | Type                  | Params | Mode \n",
      "--------------------------------------------------------------\n",
      "0 | loss_module | CrossEntropyLoss      | 0      | train\n",
      "1 | model       | GNNModel              | 1.1 K  | train\n",
      "2 | cm          | BinaryConfusionMatrix | 0      | train\n",
      "3 | aucroc      | BinaryAUROC           | 0      | train\n",
      "4 | f1          | BinaryF1Score         | 0      | train\n",
      "5 | precision   | BinaryPrecision       | 0      | train\n",
      "6 | recall      | BinaryRecall          | 0      | train\n",
      "--------------------------------------------------------------\n",
      "1.1 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.1 K     Total params\n",
      "0.005     Total estimated model params size (MB)\n",
      "31        Modules in train mode\n",
      "0         Modules in eval mode\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_layers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dropout_rate' was locked by 'sweep' (ignored update).\n",
      "d:\\winfiles\\miniconda3\\envs\\gen-disease-gnn\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "d:\\winfiles\\miniconda3\\envs\\gen-disease-gnn\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "d:\\winfiles\\miniconda3\\envs\\gen-disease-gnn\\Lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:298: The number of training batches (6) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_layers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dropout_rate' was locked by 'sweep' (ignored update).\n",
      "d:\\winfiles\\miniconda3\\envs\\gen-disease-gnn\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "d:\\winfiles\\miniconda3\\envs\\gen-disease-gnn\\Lib\\site-packages\\torchmetrics\\utilities\\prints.py:43: UserWarning: No positive samples in targets, true positive value should be meaningless. Returning zero tensor in true positive score\n",
      "  warnings.warn(*args, **kwargs)  # noqa: B028\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_acc          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">            0.0            </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       test_auc_roc        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">            0.0            </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">          test_f1          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">            0.0            </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">            0.0            </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      test_precision       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">            0.0            </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">        test_recall        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">            0.0            </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_acc         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m           0.0           \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      test_auc_roc       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m           0.0           \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m         test_f1         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m           0.0           \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m           0.0           \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m     test_precision      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m           0.0           \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m       test_recall       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m           0.0           \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "        .wandb-row {\n",
       "            display: flex;\n",
       "            flex-direction: row;\n",
       "            flex-wrap: wrap;\n",
       "            justify-content: flex-start;\n",
       "            width: 100%;\n",
       "        }\n",
       "        .wandb-col {\n",
       "            display: flex;\n",
       "            flex-direction: column;\n",
       "            flex-basis: 100%;\n",
       "            flex: 1;\n",
       "            padding: 10px;\n",
       "        }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁█</td></tr><tr><td>test_acc</td><td>▁</td></tr><tr><td>test_auc_roc</td><td>▁</td></tr><tr><td>test_f1</td><td>▁</td></tr><tr><td>test_loss</td><td>▁</td></tr><tr><td>test_precision</td><td>▁</td></tr><tr><td>test_recall</td><td>▁</td></tr><tr><td>train_acc_epoch</td><td>▁</td></tr><tr><td>train_loss_epoch</td><td>▁</td></tr><tr><td>trainer/global_step</td><td>▁▁█</td></tr><tr><td>val_acc</td><td>▁</td></tr><tr><td>val_loss</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>1</td></tr><tr><td>test_acc</td><td>0</td></tr><tr><td>test_auc_roc</td><td>0</td></tr><tr><td>test_f1</td><td>0</td></tr><tr><td>test_loss</td><td>0</td></tr><tr><td>test_precision</td><td>0</td></tr><tr><td>test_recall</td><td>0</td></tr><tr><td>train_acc_epoch</td><td>0</td></tr><tr><td>train_loss_epoch</td><td>368.5762</td></tr><tr><td>trainer/global_step</td><td>6</td></tr><tr><td>val_acc</td><td>0.18182</td></tr><tr><td>val_loss</td><td>32.51141</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">dark-sweep-4</strong> at: <a href='https://wandb.ai/nemes-attila-budapesti-m-szaki-s-gazdas-gtudom-nyi-egyetem/gnn_test_logs/runs/uhiszuws' target=\"_blank\">https://wandb.ai/nemes-attila-budapesti-m-szaki-s-gazdas-gtudom-nyi-egyetem/gnn_test_logs/runs/uhiszuws</a><br/> View project at: <a href='https://wandb.ai/nemes-attila-budapesti-m-szaki-s-gazdas-gtudom-nyi-egyetem/gnn_test_logs' target=\"_blank\">https://wandb.ai/nemes-attila-budapesti-m-szaki-s-gazdas-gtudom-nyi-egyetem/gnn_test_logs</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20241124_145547-uhiszuws\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: k8uhe2t2 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 80\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout_rate: 0.3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_layer_size: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0011444641965309478\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_layers: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adamW\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>d:\\gitrepos\\gene-disease-gnn\\wandb\\run-20241124_145552-k8uhe2t2</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/nemes-attila-budapesti-m-szaki-s-gazdas-gtudom-nyi-egyetem/gnn_test_logs/runs/k8uhe2t2' target=\"_blank\">royal-sweep-5</a></strong> to <a href='https://wandb.ai/nemes-attila-budapesti-m-szaki-s-gazdas-gtudom-nyi-egyetem/gnn_test_logs' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/nemes-attila-budapesti-m-szaki-s-gazdas-gtudom-nyi-egyetem/gnn_test_logs/sweeps/fhwx9he8' target=\"_blank\">https://wandb.ai/nemes-attila-budapesti-m-szaki-s-gazdas-gtudom-nyi-egyetem/gnn_test_logs/sweeps/fhwx9he8</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/nemes-attila-budapesti-m-szaki-s-gazdas-gtudom-nyi-egyetem/gnn_test_logs' target=\"_blank\">https://wandb.ai/nemes-attila-budapesti-m-szaki-s-gazdas-gtudom-nyi-egyetem/gnn_test_logs</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/nemes-attila-budapesti-m-szaki-s-gazdas-gtudom-nyi-egyetem/gnn_test_logs/sweeps/fhwx9he8' target=\"_blank\">https://wandb.ai/nemes-attila-budapesti-m-szaki-s-gazdas-gtudom-nyi-egyetem/gnn_test_logs/sweeps/fhwx9he8</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/nemes-attila-budapesti-m-szaki-s-gazdas-gtudom-nyi-egyetem/gnn_test_logs/runs/k8uhe2t2' target=\"_blank\">https://wandb.ai/nemes-attila-budapesti-m-szaki-s-gazdas-gtudom-nyi-egyetem/gnn_test_logs/runs/k8uhe2t2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "d:\\winfiles\\miniconda3\\envs\\gen-disease-gnn\\Lib\\site-packages\\torch_geometric\\deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "d:\\winfiles\\miniconda3\\envs\\gen-disease-gnn\\Lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:396: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "d:\\winfiles\\miniconda3\\envs\\gen-disease-gnn\\Lib\\site-packages\\pytorch_lightning\\callbacks\\model_checkpoint.py:654: Checkpoint directory D:\\gitrepos\\gene-disease-gnn\\data\\saved_models\\wandb exists and is not empty.\n",
      "\n",
      "  | Name        | Type                  | Params | Mode \n",
      "--------------------------------------------------------------\n",
      "0 | loss_module | CrossEntropyLoss      | 0      | train\n",
      "1 | model       | GNNModel              | 1.1 K  | train\n",
      "2 | cm          | BinaryConfusionMatrix | 0      | train\n",
      "3 | aucroc      | BinaryAUROC           | 0      | train\n",
      "4 | f1          | BinaryF1Score         | 0      | train\n",
      "5 | precision   | BinaryPrecision       | 0      | train\n",
      "6 | recall      | BinaryRecall          | 0      | train\n",
      "--------------------------------------------------------------\n",
      "1.1 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.1 K     Total params\n",
      "0.005     Total estimated model params size (MB)\n",
      "31        Modules in train mode\n",
      "0         Modules in eval mode\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_layers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dropout_rate' was locked by 'sweep' (ignored update).\n",
      "d:\\winfiles\\miniconda3\\envs\\gen-disease-gnn\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "d:\\winfiles\\miniconda3\\envs\\gen-disease-gnn\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "d:\\winfiles\\miniconda3\\envs\\gen-disease-gnn\\Lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:298: The number of training batches (6) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_layers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dropout_rate' was locked by 'sweep' (ignored update).\n",
      "d:\\winfiles\\miniconda3\\envs\\gen-disease-gnn\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "d:\\winfiles\\miniconda3\\envs\\gen-disease-gnn\\Lib\\site-packages\\torchmetrics\\utilities\\prints.py:43: UserWarning: No positive samples in targets, true positive value should be meaningless. Returning zero tensor in true positive score\n",
      "  warnings.warn(*args, **kwargs)  # noqa: B028\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_acc          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">            0.0            </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       test_auc_roc        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">            0.0            </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">          test_f1          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">            0.0            </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">            0.0            </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      test_precision       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">            0.0            </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">        test_recall        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">            0.0            </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_acc         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m           0.0           \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      test_auc_roc       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m           0.0           \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m         test_f1         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m           0.0           \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m           0.0           \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m     test_precision      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m           0.0           \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m       test_recall       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m           0.0           \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "        .wandb-row {\n",
       "            display: flex;\n",
       "            flex-direction: row;\n",
       "            flex-wrap: wrap;\n",
       "            justify-content: flex-start;\n",
       "            width: 100%;\n",
       "        }\n",
       "        .wandb-col {\n",
       "            display: flex;\n",
       "            flex-direction: column;\n",
       "            flex-basis: 100%;\n",
       "            flex: 1;\n",
       "            padding: 10px;\n",
       "        }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁█</td></tr><tr><td>test_acc</td><td>▁</td></tr><tr><td>test_auc_roc</td><td>▁</td></tr><tr><td>test_f1</td><td>▁</td></tr><tr><td>test_loss</td><td>▁</td></tr><tr><td>test_precision</td><td>▁</td></tr><tr><td>test_recall</td><td>▁</td></tr><tr><td>train_acc_epoch</td><td>▁</td></tr><tr><td>train_loss_epoch</td><td>▁</td></tr><tr><td>trainer/global_step</td><td>▁▁█</td></tr><tr><td>val_acc</td><td>▁</td></tr><tr><td>val_loss</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>1</td></tr><tr><td>test_acc</td><td>0</td></tr><tr><td>test_auc_roc</td><td>0</td></tr><tr><td>test_f1</td><td>0</td></tr><tr><td>test_loss</td><td>0</td></tr><tr><td>test_precision</td><td>0</td></tr><tr><td>test_recall</td><td>0</td></tr><tr><td>train_acc_epoch</td><td>0</td></tr><tr><td>train_loss_epoch</td><td>368.5762</td></tr><tr><td>trainer/global_step</td><td>6</td></tr><tr><td>val_acc</td><td>0.18182</td></tr><tr><td>val_loss</td><td>32.51141</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">royal-sweep-5</strong> at: <a href='https://wandb.ai/nemes-attila-budapesti-m-szaki-s-gazdas-gtudom-nyi-egyetem/gnn_test_logs/runs/k8uhe2t2' target=\"_blank\">https://wandb.ai/nemes-attila-budapesti-m-szaki-s-gazdas-gtudom-nyi-egyetem/gnn_test_logs/runs/k8uhe2t2</a><br/> View project at: <a href='https://wandb.ai/nemes-attila-budapesti-m-szaki-s-gazdas-gtudom-nyi-egyetem/gnn_test_logs' target=\"_blank\">https://wandb.ai/nemes-attila-budapesti-m-szaki-s-gazdas-gtudom-nyi-egyetem/gnn_test_logs</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20241124_145552-k8uhe2t2\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: bo2l41fp with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 88\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout_rate: 0.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_layer_size: 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.04616233592574638\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_layers: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>d:\\gitrepos\\gene-disease-gnn\\wandb\\run-20241124_145557-bo2l41fp</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/nemes-attila-budapesti-m-szaki-s-gazdas-gtudom-nyi-egyetem/gnn_test_logs/runs/bo2l41fp' target=\"_blank\">still-sweep-6</a></strong> to <a href='https://wandb.ai/nemes-attila-budapesti-m-szaki-s-gazdas-gtudom-nyi-egyetem/gnn_test_logs' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/nemes-attila-budapesti-m-szaki-s-gazdas-gtudom-nyi-egyetem/gnn_test_logs/sweeps/fhwx9he8' target=\"_blank\">https://wandb.ai/nemes-attila-budapesti-m-szaki-s-gazdas-gtudom-nyi-egyetem/gnn_test_logs/sweeps/fhwx9he8</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/nemes-attila-budapesti-m-szaki-s-gazdas-gtudom-nyi-egyetem/gnn_test_logs' target=\"_blank\">https://wandb.ai/nemes-attila-budapesti-m-szaki-s-gazdas-gtudom-nyi-egyetem/gnn_test_logs</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/nemes-attila-budapesti-m-szaki-s-gazdas-gtudom-nyi-egyetem/gnn_test_logs/sweeps/fhwx9he8' target=\"_blank\">https://wandb.ai/nemes-attila-budapesti-m-szaki-s-gazdas-gtudom-nyi-egyetem/gnn_test_logs/sweeps/fhwx9he8</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/nemes-attila-budapesti-m-szaki-s-gazdas-gtudom-nyi-egyetem/gnn_test_logs/runs/bo2l41fp' target=\"_blank\">https://wandb.ai/nemes-attila-budapesti-m-szaki-s-gazdas-gtudom-nyi-egyetem/gnn_test_logs/runs/bo2l41fp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "d:\\winfiles\\miniconda3\\envs\\gen-disease-gnn\\Lib\\site-packages\\torch_geometric\\deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "d:\\winfiles\\miniconda3\\envs\\gen-disease-gnn\\Lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:396: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "d:\\winfiles\\miniconda3\\envs\\gen-disease-gnn\\Lib\\site-packages\\pytorch_lightning\\callbacks\\model_checkpoint.py:654: Checkpoint directory D:\\gitrepos\\gene-disease-gnn\\data\\saved_models\\wandb exists and is not empty.\n",
      "\n",
      "  | Name        | Type                  | Params | Mode \n",
      "--------------------------------------------------------------\n",
      "0 | loss_module | CrossEntropyLoss      | 0      | train\n",
      "1 | model       | GNNModel              | 3.8 K  | train\n",
      "2 | cm          | BinaryConfusionMatrix | 0      | train\n",
      "3 | aucroc      | BinaryAUROC           | 0      | train\n",
      "4 | f1          | BinaryF1Score         | 0      | train\n",
      "5 | precision   | BinaryPrecision       | 0      | train\n",
      "6 | recall      | BinaryRecall          | 0      | train\n",
      "--------------------------------------------------------------\n",
      "3.8 K     Trainable params\n",
      "0         Non-trainable params\n",
      "3.8 K     Total params\n",
      "0.015     Total estimated model params size (MB)\n",
      "56        Modules in train mode\n",
      "0         Modules in eval mode\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_layers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dropout_rate' was locked by 'sweep' (ignored update).\n",
      "d:\\winfiles\\miniconda3\\envs\\gen-disease-gnn\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "d:\\winfiles\\miniconda3\\envs\\gen-disease-gnn\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "d:\\winfiles\\miniconda3\\envs\\gen-disease-gnn\\Lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:298: The number of training batches (6) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_layers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dropout_rate' was locked by 'sweep' (ignored update).\n",
      "d:\\winfiles\\miniconda3\\envs\\gen-disease-gnn\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "d:\\winfiles\\miniconda3\\envs\\gen-disease-gnn\\Lib\\site-packages\\torchmetrics\\utilities\\prints.py:43: UserWarning: No positive samples in targets, true positive value should be meaningless. Returning zero tensor in true positive score\n",
      "  warnings.warn(*args, **kwargs)  # noqa: B028\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_acc          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">            0.0            </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       test_auc_roc        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">            0.0            </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">          test_f1          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">            0.0            </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">            0.0            </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      test_precision       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">            0.0            </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">        test_recall        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">            0.0            </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_acc         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m           0.0           \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      test_auc_roc       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m           0.0           \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m         test_f1         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m           0.0           \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m           0.0           \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m     test_precision      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m           0.0           \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m       test_recall       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m           0.0           \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "        .wandb-row {\n",
       "            display: flex;\n",
       "            flex-direction: row;\n",
       "            flex-wrap: wrap;\n",
       "            justify-content: flex-start;\n",
       "            width: 100%;\n",
       "        }\n",
       "        .wandb-col {\n",
       "            display: flex;\n",
       "            flex-direction: column;\n",
       "            flex-basis: 100%;\n",
       "            flex: 1;\n",
       "            padding: 10px;\n",
       "        }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁█</td></tr><tr><td>test_acc</td><td>▁</td></tr><tr><td>test_auc_roc</td><td>▁</td></tr><tr><td>test_f1</td><td>▁</td></tr><tr><td>test_loss</td><td>▁</td></tr><tr><td>test_precision</td><td>▁</td></tr><tr><td>test_recall</td><td>▁</td></tr><tr><td>train_acc_epoch</td><td>▁</td></tr><tr><td>train_loss_epoch</td><td>▁</td></tr><tr><td>trainer/global_step</td><td>▁▁█</td></tr><tr><td>val_acc</td><td>▁</td></tr><tr><td>val_loss</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>1</td></tr><tr><td>test_acc</td><td>0</td></tr><tr><td>test_auc_roc</td><td>0</td></tr><tr><td>test_f1</td><td>0</td></tr><tr><td>test_loss</td><td>0</td></tr><tr><td>test_precision</td><td>0</td></tr><tr><td>test_recall</td><td>0</td></tr><tr><td>train_acc_epoch</td><td>0</td></tr><tr><td>train_loss_epoch</td><td>78.86081</td></tr><tr><td>trainer/global_step</td><td>6</td></tr><tr><td>val_acc</td><td>0</td></tr><tr><td>val_loss</td><td>4.92917</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">still-sweep-6</strong> at: <a href='https://wandb.ai/nemes-attila-budapesti-m-szaki-s-gazdas-gtudom-nyi-egyetem/gnn_test_logs/runs/bo2l41fp' target=\"_blank\">https://wandb.ai/nemes-attila-budapesti-m-szaki-s-gazdas-gtudom-nyi-egyetem/gnn_test_logs/runs/bo2l41fp</a><br/> View project at: <a href='https://wandb.ai/nemes-attila-budapesti-m-szaki-s-gazdas-gtudom-nyi-egyetem/gnn_test_logs' target=\"_blank\">https://wandb.ai/nemes-attila-budapesti-m-szaki-s-gazdas-gtudom-nyi-egyetem/gnn_test_logs</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20241124_145557-bo2l41fp\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 3djhwu1o with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 264\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout_rate: 0.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_layer_size: 30\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.005608682935891474\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_layers: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adamW\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>d:\\gitrepos\\gene-disease-gnn\\wandb\\run-20241124_145603-3djhwu1o</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/nemes-attila-budapesti-m-szaki-s-gazdas-gtudom-nyi-egyetem/gnn_test_logs/runs/3djhwu1o' target=\"_blank\">balmy-sweep-7</a></strong> to <a href='https://wandb.ai/nemes-attila-budapesti-m-szaki-s-gazdas-gtudom-nyi-egyetem/gnn_test_logs' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/nemes-attila-budapesti-m-szaki-s-gazdas-gtudom-nyi-egyetem/gnn_test_logs/sweeps/fhwx9he8' target=\"_blank\">https://wandb.ai/nemes-attila-budapesti-m-szaki-s-gazdas-gtudom-nyi-egyetem/gnn_test_logs/sweeps/fhwx9he8</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/nemes-attila-budapesti-m-szaki-s-gazdas-gtudom-nyi-egyetem/gnn_test_logs' target=\"_blank\">https://wandb.ai/nemes-attila-budapesti-m-szaki-s-gazdas-gtudom-nyi-egyetem/gnn_test_logs</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/nemes-attila-budapesti-m-szaki-s-gazdas-gtudom-nyi-egyetem/gnn_test_logs/sweeps/fhwx9he8' target=\"_blank\">https://wandb.ai/nemes-attila-budapesti-m-szaki-s-gazdas-gtudom-nyi-egyetem/gnn_test_logs/sweeps/fhwx9he8</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/nemes-attila-budapesti-m-szaki-s-gazdas-gtudom-nyi-egyetem/gnn_test_logs/runs/3djhwu1o' target=\"_blank\">https://wandb.ai/nemes-attila-budapesti-m-szaki-s-gazdas-gtudom-nyi-egyetem/gnn_test_logs/runs/3djhwu1o</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "d:\\winfiles\\miniconda3\\envs\\gen-disease-gnn\\Lib\\site-packages\\torch_geometric\\deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "d:\\winfiles\\miniconda3\\envs\\gen-disease-gnn\\Lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:396: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "d:\\winfiles\\miniconda3\\envs\\gen-disease-gnn\\Lib\\site-packages\\pytorch_lightning\\callbacks\\model_checkpoint.py:654: Checkpoint directory D:\\gitrepos\\gene-disease-gnn\\data\\saved_models\\wandb exists and is not empty.\n",
      "\n",
      "  | Name        | Type                  | Params | Mode \n",
      "--------------------------------------------------------------\n",
      "0 | loss_module | CrossEntropyLoss      | 0      | train\n",
      "1 | model       | GNNModel              | 8.0 K  | train\n",
      "2 | cm          | BinaryConfusionMatrix | 0      | train\n",
      "3 | aucroc      | BinaryAUROC           | 0      | train\n",
      "4 | f1          | BinaryF1Score         | 0      | train\n",
      "5 | precision   | BinaryPrecision       | 0      | train\n",
      "6 | recall      | BinaryRecall          | 0      | train\n",
      "--------------------------------------------------------------\n",
      "8.0 K     Trainable params\n",
      "0         Non-trainable params\n",
      "8.0 K     Total params\n",
      "0.032     Total estimated model params size (MB)\n",
      "56        Modules in train mode\n",
      "0         Modules in eval mode\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_layers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dropout_rate' was locked by 'sweep' (ignored update).\n",
      "d:\\winfiles\\miniconda3\\envs\\gen-disease-gnn\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "d:\\winfiles\\miniconda3\\envs\\gen-disease-gnn\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "d:\\winfiles\\miniconda3\\envs\\gen-disease-gnn\\Lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:298: The number of training batches (6) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_layers' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dropout_rate' was locked by 'sweep' (ignored update).\n",
      "d:\\winfiles\\miniconda3\\envs\\gen-disease-gnn\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "d:\\winfiles\\miniconda3\\envs\\gen-disease-gnn\\Lib\\site-packages\\torchmetrics\\utilities\\prints.py:43: UserWarning: No positive samples in targets, true positive value should be meaningless. Returning zero tensor in true positive score\n",
      "  warnings.warn(*args, **kwargs)  # noqa: B028\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_acc          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">            0.0            </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       test_auc_roc        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">            0.0            </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">          test_f1          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">            0.0            </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">            0.0            </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      test_precision       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">            0.0            </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">        test_recall        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">            0.0            </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_acc         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m           0.0           \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      test_auc_roc       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m           0.0           \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m         test_f1         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m           0.0           \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m           0.0           \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m     test_precision      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m           0.0           \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m       test_recall       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m           0.0           \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "        .wandb-row {\n",
       "            display: flex;\n",
       "            flex-direction: row;\n",
       "            flex-wrap: wrap;\n",
       "            justify-content: flex-start;\n",
       "            width: 100%;\n",
       "        }\n",
       "        .wandb-col {\n",
       "            display: flex;\n",
       "            flex-direction: column;\n",
       "            flex-basis: 100%;\n",
       "            flex: 1;\n",
       "            padding: 10px;\n",
       "        }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁█</td></tr><tr><td>test_acc</td><td>▁</td></tr><tr><td>test_auc_roc</td><td>▁</td></tr><tr><td>test_f1</td><td>▁</td></tr><tr><td>test_loss</td><td>▁</td></tr><tr><td>test_precision</td><td>▁</td></tr><tr><td>test_recall</td><td>▁</td></tr><tr><td>train_acc_epoch</td><td>▁</td></tr><tr><td>train_loss_epoch</td><td>▁</td></tr><tr><td>trainer/global_step</td><td>▁▁█</td></tr><tr><td>val_acc</td><td>▁</td></tr><tr><td>val_loss</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>1</td></tr><tr><td>test_acc</td><td>0</td></tr><tr><td>test_auc_roc</td><td>0</td></tr><tr><td>test_f1</td><td>0</td></tr><tr><td>test_loss</td><td>0</td></tr><tr><td>test_precision</td><td>0</td></tr><tr><td>test_recall</td><td>0</td></tr><tr><td>train_acc_epoch</td><td>0</td></tr><tr><td>train_loss_epoch</td><td>35.88692</td></tr><tr><td>trainer/global_step</td><td>6</td></tr><tr><td>val_acc</td><td>0</td></tr><tr><td>val_loss</td><td>4.69136</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">balmy-sweep-7</strong> at: <a href='https://wandb.ai/nemes-attila-budapesti-m-szaki-s-gazdas-gtudom-nyi-egyetem/gnn_test_logs/runs/3djhwu1o' target=\"_blank\">https://wandb.ai/nemes-attila-budapesti-m-szaki-s-gazdas-gtudom-nyi-egyetem/gnn_test_logs/runs/3djhwu1o</a><br/> View project at: <a href='https://wandb.ai/nemes-attila-budapesti-m-szaki-s-gazdas-gtudom-nyi-egyetem/gnn_test_logs' target=\"_blank\">https://wandb.ai/nemes-attila-budapesti-m-szaki-s-gazdas-gtudom-nyi-egyetem/gnn_test_logs</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20241124_145603-3djhwu1o\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.agent(sweep_id=sweep_id, function=optimalization_train, count=5)\n",
    "wandb.teardown()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "gen-disease-gnn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
